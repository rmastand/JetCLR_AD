{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e80fafd-9a03-435d-a5eb-d6132a9012e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# load standard python modules\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# load torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from modules.file_readers import phi_wrap, pandas_to_unclustered_particles, get_highest_mass_constituents, pandas_to_features, select_jets\n",
    "from modules.jet_augs import crop_jets, rescale_pts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93478edd-8830-46e1-ab73-83083fe8cd9f",
   "metadata": {},
   "source": [
    "# Load in the data\n",
    "\n",
    "Starts with a dataset of particles $p_T$, $\\eta$, $\\phi$\n",
    "\n",
    "Clusters into jets, takes highest mass jet, returns the constituents ordered by pT\n",
    "\n",
    "Dataset shape: (n,3,101) = (n,[$p_T$, $\\eta$, $\\phi$],1 jet + 100 constituents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f073bead-8e91-4bcc-afb2-34cb13156afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_npy_exists = True\n",
    "\n",
    "path_to_unclustered = \"/clusterfs/ml4hep/rrmastandrea/LHC0/events_anomalydetection.h5\"\n",
    "path_to_data_storage = \"/clusterfs/ml4hep/rrmastandrea/processed_data/02092021/\"\n",
    "\n",
    "if not clustered_npy_exists:\n",
    "    \n",
    "    start = 95000\n",
    "    stop = 100000\n",
    "    jetR = 0.8\n",
    "    \n",
    "    fname_data = \"highmassconstits_data_jetR_\"+str(jetR)+\"_deltaJ_\"+str(start)+\"_\"+str(stop)+\".npy\"\n",
    "    fname_labels = \"highmassconstits_labels_jetR_\"+str(jetR)+\"_deltaJ_\"+str(start)+\"_\"+str(stop)+\".npy\"\n",
    "    \n",
    "    print(\"Reading in unclustered events...\")\n",
    "    # Read in the file\n",
    "    unclustered_particles_data = pd.read_hdf(path_to_unclustered,start = start, stop = stop)\n",
    "    # Convert pd to numpy; get labels\n",
    "    unclustered_collisions, unclustered_particles_labels = pandas_to_unclustered_particles(unclustered_particles_data)\n",
    "\n",
    "    # Cluster into jets, get highest mass constituents\n",
    "    high_mass_consits, bad_indices = get_highest_mass_constituents(unclustered_collisions, jetR, deltaJ = True, ncon_store=101)\n",
    "\n",
    "    unclustered_particles_labels = np.delete(unclustered_particles_labels,bad_indices)\n",
    "    np.save(path_to_data_storage+fname_data, high_mass_consits)\n",
    "    np.save(path_to_data_storage+fname_labels, unclustered_particles_labels)\n",
    "    print(\"Saved file \"+fname_data)\n",
    "    print(\"Saved file \"+fname_labels)\n",
    "    \n",
    "    print(high_mass_consits.shape,unclustered_particles_labels.shape)\n",
    "\n",
    "    \n",
    "if clustered_npy_exists: \n",
    "    \n",
    "    jetR = 0.8\n",
    "    \"\"\"\n",
    "    starts_and_stops = [(0,2000),(2000,4000),(4000,6000),(6000,8000),(8000,10000),\n",
    "                       (10000,12000),(12000,14000),(14000,16000),(16000,18000),(18000,20000),\n",
    "                       (20000,22000),(22000,24000),(24000,26000),(26000,28000),(28000,30000),\n",
    "                       (30000,32000),(32000,34000),(34000,36000),(36000,38000),(38000,40000)]\n",
    "    \"\"\"\n",
    "    \n",
    "    starts_and_stops = [(0,5000),(5000,10000),(10000,15000),(15000,20000),\n",
    "                       (20000,25000),(25000,30000),(30000,35000),(35000,40000),\n",
    "                       (40000,45000),(45000,50000)]#,(50000,55000),(55000,60000),\n",
    "                       #(60000,65000),(65000,70000),(70000,75000),(75000,80000),\n",
    "                       #(80000,85000),(85000,90000),(90000,95000),(95000,100000)]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    high_mass_consits_wjet = np.load(path_to_data_storage+\"highmassconstits_data_jetR_\"+str(jetR)+\"_deltaJ_\"+str(starts_and_stops[0][0])+\"_\"+str(starts_and_stops[0][1])+\".npy\")\n",
    "    high_mass_labels = np.load(path_to_data_storage+\"highmassconstits_labels_jetR_\"+str(jetR)+\"_deltaJ_\"+str(starts_and_stops[0][0])+\"_\"+str(starts_and_stops[0][1])+\".npy\")\n",
    "\n",
    "    for ss in starts_and_stops[1:]:\n",
    "        high_mass_consits_wjet = np.concatenate([high_mass_consits_wjet, np.load(path_to_data_storage+\"highmassconstits_data_jetR_\"+str(jetR)+\"_deltaJ_\"+str(ss[0])+\"_\"+str(ss[1])+\".npy\")])\n",
    "        high_mass_labels = np.concatenate([high_mass_labels, np.load(path_to_data_storage+\"highmassconstits_labels_jetR_\"+str(jetR)+\"_deltaJ_\"+str(ss[0])+\"_\"+str(ss[1])+\".npy\")])\n",
    "        \n",
    "    print(\"Read in files\")\n",
    "    print(\"Data shape: \",high_mass_consits_wjet.shape)\n",
    "    print(\"Labels shape:\", high_mass_labels.shape)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8316a-1524-4b23-a700-57e3d7991fcc",
   "metadata": {},
   "source": [
    "## Jet cuts \n",
    "\n",
    "Cut on $p_t$, $\\eta$ of the jets \n",
    "\n",
    "Then split into signal and background datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556be817-36d9-4bfc-a81d-8ca1787e6ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select jets of interest\n",
    "\n",
    "# Make the cut\n",
    "pt_cut = [1000,2000]\n",
    "eta_cut = [-2,2]\n",
    "high_mass_consits_wjet_cut, high_mass_labels_cut = select_jets(high_mass_consits_wjet, high_mass_labels, pt_cut, eta_cut)\n",
    "\n",
    "\n",
    "# plot the jet parameters\n",
    "plt.figure()\n",
    "plt.hist(high_mass_consits_wjet[:,0,0], bins = np.linspace(0,3000,60), alpha = 0.4, label = \"PRE\")\n",
    "plt.hist(high_mass_consits_wjet_cut[:,0,0], bins = np.linspace(0,3000,60), alpha = 0.4, label = \"POST\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Jet $p_T$ [GeV]\")\n",
    "plt.show()\n",
    "\n",
    "# plot the jet parameters\n",
    "plt.figure()\n",
    "plt.hist(high_mass_consits_wjet[:,1,0], bins = np.linspace(-3,3,60), alpha = 0.4, label = \"PRE\")\n",
    "plt.hist(high_mass_consits_wjet_cut[:,1,0], bins = np.linspace(-3,3,60), alpha = 0.4, label = \"POST\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Jet $eta$\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e864bfd",
   "metadata": {},
   "source": [
    "# Last data preprocessing\n",
    "\n",
    "-- removing the jet from the jet + constituents array\n",
    "\n",
    "-- rescaling the pt\n",
    "\n",
    "-- splitting into signal vs background\n",
    "\n",
    "-- crop the jets\n",
    "\n",
    "-- adding zero pad for collinear splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f002c-4bb0-4261-9546-c9de0d8c9629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only the constituents (i.e. drop the 0th, which is the jet)\n",
    "high_mass_consits = high_mass_consits_wjet_cut[:,:,1:]\n",
    "\n",
    "# get max, min pt for normalizing\n",
    "max_pt = np.max(high_mass_consits[:,0,:])\n",
    "max_eta = np.max(high_mass_consits[:,1,:])\n",
    "max_phi = np.max(high_mass_consits[:,2,:])\n",
    "                  \n",
    "#print(\"Max pt:\",max_pt,\"; max eta:\", max_eta, \"; max phi:\", max_phi)\n",
    "\n",
    "# rescale the pts here\n",
    "\n",
    "rescale_denom_pt = max_pt/10\n",
    "print(\"Rescaling all pTs by\",rescale_denom_pt)\n",
    "high_mass_consits = rescale_pts( high_mass_consits, rescale_denom_pt ) \n",
    "\n",
    "# crop the n constituents\n",
    "n_nonzero_conts = 2\n",
    "print(\"Cropping the jets to\",n_nonzero_conts,\"consituents\")\n",
    "high_mass_consits = crop_jets(high_mass_consits,n_nonzero_conts)\n",
    "\n",
    "# add zero pad\n",
    "n_zero_pad = 48\n",
    "print(\"Adding a zero pad of size\", n_zero_pad)\n",
    "\n",
    "def zero_pad_consts(data, n):\n",
    "    zero_pad = np.zeros((data.shape[0],3,n-data.shape[2]))\n",
    "    data = np.concatenate((data,zero_pad), axis = 2)\n",
    "    return data\n",
    "\n",
    "high_mass_consits = zero_pad_consts(high_mass_consits,n_zero_pad+n_nonzero_conts)\n",
    "\n",
    "# split into signal vs background\n",
    "\n",
    "high_mass_consits_sig = high_mass_consits[np.where(high_mass_labels_cut==1)]\n",
    "high_mass_consits_bkg = high_mass_consits[np.where(high_mass_labels_cut==0)]\n",
    "\n",
    "\n",
    "print(\"Signal shape:\", high_mass_consits_sig.shape)\n",
    "print(\"Background shape:\", high_mass_consits_bkg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5fe219-708a-4073-9a5b-94b786932c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of pt, eta, phi for the constituents\n",
    "\n",
    "N_start = 0  # Number of collision events\n",
    "N_stop = 70000\n",
    "\n",
    "M = 20  # Number of constituents\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "loc_plot = np.reshape(high_mass_consits_sig[N_start:N_stop,0,0:M],((N_start-N_stop)*(M),))\n",
    "plt.hist(loc_plot[loc_plot != 0]/1.0, bins = np.linspace(0,3,50), alpha = .3, density=True, label = \"signal\")\n",
    "loc_plot = np.reshape(high_mass_consits_bkg[N_start:N_stop,0,0:M],((N_start-N_stop)*(M),))\n",
    "plt.hist(loc_plot[loc_plot != 0]/1.0, bins = np.linspace(0,3,50), alpha = .3, density=True, label = \"background\")\n",
    "plt.xlabel(\"Constituent $p_T (Rescaled)$\")\n",
    "#plt.xlim(0,5000)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "loc_plot = np.reshape(high_mass_consits_sig[N_start:N_stop,1,0:M],((N_start-N_stop)*(M),))\n",
    "plt.hist(loc_plot[loc_plot != 0], alpha = .3, density=True, label = \"signal\")\n",
    "loc_plot = np.reshape(high_mass_consits_bkg[N_start:N_stop,1,0:M],((N_start-N_stop)*(M),))\n",
    "plt.hist(loc_plot[loc_plot != 0], alpha = .3, density=True, label = \"background\")\n",
    "plt.xlabel(\"Constituent $\\eta-\\eta_J$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "loc_plot = np.reshape(high_mass_consits_sig[N_start:N_stop,2,0:M],((N_start-N_stop)*(M),))\n",
    "plt.hist(loc_plot[loc_plot != 0], alpha = .3, density=True, label = \"signal\")\n",
    "loc_plot = np.reshape(high_mass_consits_bkg[N_start:N_stop,2,0:M],((N_start-N_stop)*(M),))\n",
    "plt.hist(loc_plot[loc_plot != 0], alpha = .3, density=True, label = \"background\")\n",
    "plt.xlabel(\"Constituent $\\phi-\\phi_J$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot num constituents\n",
    "\n",
    "def get_num_constits(dataset):\n",
    "    consits_list = []\n",
    "    for collision in dataset:\n",
    "        pts = collision[0,:]\n",
    "\n",
    "        pads = np.where(pts==0)\n",
    "        consits_list.append(dataset.shape[2]-len(pads[0]))\n",
    "        \n",
    "    return consits_list\n",
    "        \n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(get_num_constits(high_mass_consits_sig), alpha = .3, density=True, label = \"signal\")\n",
    "plt.hist(get_num_constits(high_mass_consits_bkg), alpha = .3, density=True, label = \"background\")\n",
    "plt.xlabel(\"Num. constituents\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed59a4e-3247-49a2-909c-d9bac396cab1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Split the data into test / train / validation sets\n",
    "\n",
    "- CLR training: First ```num_clr_train``` background events\n",
    "- CLR val: 20% of CLR train\n",
    "\n",
    "- Classification training: (up to) ```njets_sig``` signal, ```njets_bkg``` background\n",
    "- Classification test: 30% of training\n",
    "- Classification test is further split into validation / test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d14a7-86b8-4f05-8e9a-546ece4a9b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dim to the transformer -> (pt,eta,phi)\n",
    "input_dim = 3\n",
    "\n",
    "# creating the training dataset\n",
    "\n",
    "n_constits_max = n_nonzero_conts\n",
    "\n",
    "\"\"\"\n",
    "RUN THE JET SELECTOR CODE\n",
    "\"\"\"\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "\"\"\"\n",
    "Make the CLR train and val datasets\n",
    "\"\"\"\n",
    "num_clr_train_sig = 5000\n",
    "num_clr_train_bkg = 8000\n",
    "val_size = 0.2\n",
    "dataset_clr_sig = high_mass_consits_sig[0:num_clr_train_sig,:,:]\n",
    "dataset_clr_bkg = high_mass_consits_bkg[0:num_clr_train_bkg,:,:]\n",
    "\n",
    "\n",
    "# split into train - val\n",
    "((clr_sig_train, clr_sig_val),\n",
    " (clr_bkg_train, clr_bkg_val),\n",
    " ) = [train_test_split(arr, test_size=val_size) for arr in [\n",
    "    dataset_clr_sig,\n",
    "    dataset_clr_bkg,\n",
    "]]\n",
    "\n",
    "# preparing the training dataset w/ labels\n",
    "clr_train = np.concatenate([clr_sig_train,clr_bkg_train])\n",
    "clr_train = shuffle(clr_train)\n",
    "\n",
    "# preparing the test dataset(s)\n",
    "clr_val = np.concatenate([clr_sig_val,clr_bkg_val])\n",
    "clr_val = shuffle(clr_val)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Make the classification datasets\n",
    "\"\"\"\n",
    "njets_sig = 5000\n",
    "njets_bkg = 8000\n",
    "\n",
    "# take only however many constituents and jets we want \n",
    "dataset_sample_sig = high_mass_consits_sig[0:njets_sig,:,:]\n",
    "dataset_sample_bkg = high_mass_consits_bkg[0:njets_bkg,:,:]\n",
    "\n",
    "\n",
    "print(\"Num signal:\", dataset_sample_sig.shape[0],\"; Num background:\", dataset_sample_bkg.shape[0])\n",
    "\n",
    "# split into train - val \n",
    "test_size = .3\n",
    "((sig_train, sig_test),\n",
    " (bkg_train, bkg_test),\n",
    " ) = [train_test_split(arr, test_size=test_size) for arr in [\n",
    "    dataset_sample_sig,\n",
    "    dataset_sample_bkg,\n",
    "]]\n",
    "\n",
    "# preparing the training dataset w/ labels\n",
    "data_train = np.concatenate([sig_train,bkg_train])\n",
    "labels_train = np.concatenate([np.ones(sig_train.shape[0]),np.zeros(bkg_train.shape[0])])\n",
    "data_train, labels_train = shuffle(data_train, labels_train)\n",
    "\n",
    "# preparing the test dataset(s)\n",
    "data_test = np.concatenate([sig_test,bkg_test])\n",
    "labels_test = np.concatenate([np.ones(sig_test.shape[0]),np.zeros(bkg_test.shape[0])])\n",
    "data_test, labels_test = shuffle(data_test, labels_test)\n",
    "\n",
    "# Split the test into val + \"testf\"\n",
    "lct_val_size = .5\n",
    "n_val = int(data_test.shape[0]*lct_val_size)\n",
    "data_val = data_test[0:n_val,:,:]\n",
    "labels_val = labels_test[0:n_val]\n",
    "data_test_f = data_test[-n_val:,:,:]\n",
    "labels_test_f = labels_test[-n_val:]\n",
    "\n",
    "# print data dimensions\n",
    "print( \"CLR training data shape: \" + str( clr_train.shape ), flush=True)\n",
    "print( \"CLR val data shape: \" + str( clr_val.shape ), flush=True)\n",
    "print( \"BC training data shape: \" + str( data_train.shape ), flush=True)\n",
    "print( \"BC training labels shape: \" + str( labels_train.shape ), flush=True)\n",
    "print( \"BC val data shape: \" + str( data_val.shape ), flush=True)\n",
    "print( \"BC val labels shape: \" + str( labels_val.shape ), flush=True)\n",
    "print( \"BC test data shape: \" + str( data_test_f.shape ), flush=True)\n",
    "print( \"BC test labels shape: \" + str( labels_test_f.shape ), flush=True)\n",
    "\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print( \"time taken to load and preprocess data: \"+str( np.round( t1-t0, 2 ) ) + \" seconds\", flush=True  )\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1ad8f-d110-4896-b89e-e69e603adaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_dir = \"/global/home/users/rrmastandrea/training_data/\"\n",
    "\n",
    "\n",
    "n_sig_total = high_mass_consits_sig.shape[0]\n",
    "n_bkg_total = high_mass_consits_bkg.shape[0]\n",
    "\n",
    "\n",
    "save_id_dir = \"n_sig_\"+str(n_sig_total)+\"_n_bkg_\"+str(n_bkg_total)+\"_n_nonzero_\"+str(n_nonzero_conts)+\"_n_pad_\"+str(n_zero_pad)+\"/\"\n",
    "\n",
    "path_to_save_dir += save_id_dir\n",
    "print(path_to_save_dir)\n",
    "\n",
    "if os.path.isdir(path_to_save_dir):\n",
    "    print(\"ERROR: experiment already exists, don't want to overwrite it by mistake\")\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(path_to_save_dir)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "np.save(path_to_save_dir+\"clr_train.npy\",clr_train)\n",
    "np.save(path_to_save_dir+\"clr_val.npy\",clr_val)\n",
    "np.save(path_to_save_dir+\"data_train.npy\",data_train)\n",
    "np.save(path_to_save_dir+\"labels_train.npy\",labels_train)\n",
    "np.save(path_to_save_dir+\"data_val.npy\",data_val)\n",
    "np.save(path_to_save_dir+\"labels_val.npy\",labels_val)\n",
    "np.save(path_to_save_dir+\"data_test_f.npy\",data_test_f)\n",
    "np.save(path_to_save_dir+\"labels_test_f.npy\",labels_test_f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca05eca-25ef-4c41-94bb-404aaae36882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffee9e5-937c-475d-ab53-ba53f2f6dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b948293-9a47-477f-a50a-9cb1375a1a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d393ec-4032-4883-9c2b-f59549a851d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f88324d-16ea-4802-8f72-acd1637ee254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5b18c-c754-4d6f-96e2-e6ceb3ea72c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
