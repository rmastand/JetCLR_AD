{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e80fafd-9a03-435d-a5eb-d6132a9012e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# load standard python modules\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# load torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# load custom modules required for jetCLR training\n",
    "from modules.jet_augs import apply_single_jet_augs, translate_jets, rotate_jets, rescale_pts, distort_jets, collinear_fill_jets, crop_jets\n",
    "from modules.jet_augs import shift_eta, shift_phi\n",
    "from modules.transformer import Transformer\n",
    "from modules.losses import contrastive_loss, align_loss, uniform_loss, contrastive_loss_num_den\n",
    "from modules.perf_eval import get_perf_stats, linear_classifier_test, plot_losses\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c7ac9c-3b98-4534-bb70-66b0aa2d4f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "ERROR: experiment already exists, don't want to overwrite it by mistake\n",
      "experiment: dijet/test0/\n"
     ]
    }
   ],
   "source": [
    "# More parameters / computing setup\n",
    "\n",
    "# set the number of threads that pytorch will use\n",
    "torch.set_num_threads(2)\n",
    "\n",
    "exp_id = \"dijet/test0/\"\n",
    "\n",
    "# set gpu device\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print( \"device: \" + str( device ), flush=True)\n",
    "\n",
    "# set up results directory\n",
    "base_dir = \"/global/home/users/rrmastandrea/MJetCLR/\"  # change this to your working directory\n",
    "expt_dir = base_dir + \"projects/rep_learning/experiments/\" + exp_id + \"/\"\n",
    "\n",
    "#check if experiment alreadyexists\n",
    "if os.path.isdir(expt_dir):\n",
    "    print(\"ERROR: experiment already exists, don't want to overwrite it by mistake\")\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(expt_dir)\n",
    "\n",
    "print(\"experiment: \"+str(exp_id) , flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f768aa5",
   "metadata": {},
   "source": [
    "# Load in the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5078731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/home/users/rrmastandrea/training_data/n_sig_1742_n_bkg_4000_n_nonzero_50_n_pad_0_n_jet_2/\n",
      "CLR training data shape: (7793, 3, 102)\n",
      "CLR val data shape: (1949, 3, 102)\n",
      "BC training data shape: (4019, 3, 102)\n",
      "BC training labels shape: (4019,)\n",
      "BC val data shape: (861, 3, 102)\n",
      "BC val labels shape: (861,)\n",
      "BC test data shape: (861, 3, 102)\n",
      "BC test labels shape: (861,)\n"
     ]
    }
   ],
   "source": [
    "path_to_save_dir = \"/global/home/users/rrmastandrea/training_data/\"\n",
    "\n",
    "\n",
    "#save_id_dir = \"n_sig_6557_n_bkg_8000_n_nonzero_50_n_pad_0/\"\n",
    "save_id_dir = \"n_sig_1742_n_bkg_4000_n_nonzero_50_n_pad_0_n_jet_2/\"\n",
    "\n",
    "grading = 50\n",
    "\n",
    "path_to_data = path_to_save_dir+save_id_dir\n",
    "print(path_to_data)\n",
    "\n",
    "\n",
    "clr_train = np.load(path_to_data+\"clr_train.npy\")\n",
    "clr_val = np.load(path_to_data+\"clr_val.npy\")\n",
    "data_train = np.load(path_to_data+\"data_train.npy\")\n",
    "labels_train = np.load(path_to_data+\"labels_train.npy\")\n",
    "data_val = np.load(path_to_data+\"data_val.npy\")\n",
    "labels_val = np.load(path_to_data+\"labels_val.npy\")\n",
    "data_test_f = np.load(path_to_data+\"data_test_f.npy\")\n",
    "labels_test_f = np.load(path_to_data+\"labels_test_f.npy\")\n",
    "\n",
    "# print data dimensions\n",
    "print( \"CLR training data shape: \" + str( clr_train.shape ), flush=True)\n",
    "print( \"CLR val data shape: \" + str( clr_val.shape ), flush=True)\n",
    "print( \"BC training data shape: \" + str( data_train.shape ), flush=True)\n",
    "print( \"BC training labels shape: \" + str( labels_train.shape ), flush=True)\n",
    "print( \"BC val data shape: \" + str( data_val.shape ), flush=True)\n",
    "print( \"BC val labels shape: \" + str( labels_val.shape ), flush=True)\n",
    "print( \"BC test data shape: \" + str( data_test_f.shape ), flush=True)\n",
    "print( \"BC test labels shape: \" + str( labels_test_f.shape ), flush=True)\n",
    "\n",
    "\n",
    "# Plot num constituents\n",
    "\n",
    "def get_num_constits(dataset):\n",
    "    consits_list = []\n",
    "    for collision in dataset:\n",
    "        pts = collision[0,:]\n",
    "\n",
    "        pads = np.where(pts==0)\n",
    "        consits_list.append(dataset.shape[2]-len(pads[0]))\n",
    "        \n",
    "    return consits_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e82160-2164-4206-9ea1-76cb130c0567",
   "metadata": {},
   "source": [
    "# Define the Transformer Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f66a6402-43b2-42d7-b387-a812915ccaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the transformer net\n",
    "\"\"\"\n",
    "\n",
    "# transformer hyperparams\n",
    "# input dim to the transformer -> (pt,eta,phi)\n",
    "input_dim = 3\n",
    "model_dim = 500\n",
    "output_dim = model_dim\n",
    "dim_feedforward = model_dim\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "n_head_layers = 2\n",
    "opt = \"adam\"\n",
    "\n",
    "mask= False\n",
    "cmask = True\n",
    "\n",
    "learning_rate_trans = 0.0001\n",
    "batch_size = 256\n",
    "temperature = .8\n",
    "\n",
    "# augmentations\n",
    "rot = True # rotations\n",
    "trs = True # translations\n",
    "dis = True # distortion\n",
    "col = True # collinear\n",
    "\n",
    "center = \"J1_phi_only_pi_2\"\n",
    "\n",
    "\n",
    "#rescale = 1.0\n",
    "\n",
    "net = Transformer( input_dim, model_dim, output_dim, n_heads, dim_feedforward, \n",
    "                  n_layers, learning_rate_trans, n_head_layers, dropout=0.1, opt=opt )\n",
    "\n",
    "## send network to device\n",
    "net.to( device )\n",
    "\n",
    "# define lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( net.optimizer, factor=0.2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2838b294-8030-4264-9fc5-e787408bc9f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training loop, running for 20 epochs with 50 constituents\n",
      "Training data shape: (7793, 3, 102)\n",
      "Validation data shape: (1949, 3, 102)\n",
      "---\n",
      "tensor([3382, 1102,   13, 4108, 1250,  337, 4191, 3701, 1460, 1996, 6750, 3438,\n",
      "         366, 2385,  855, 5414, 5942, 7725, 6290, 6921, 3846, 4900,  833, 1925,\n",
      "        3332, 1022, 2879, 1067, 3288, 1145, 2056, 1662, 7419,  961, 3996, 2423,\n",
      "        5237, 4718, 1941, 4950,  870,  486, 6630, 3209, 6771, 6797, 4264, 6349,\n",
      "        6660, 7546, 1476, 1064, 6179,  110, 5157, 5155, 6483, 3727, 2466, 2164,\n",
      "        7194, 6112, 4898, 6706,  628, 1983, 3803,  805, 6553,  632, 2724, 2130,\n",
      "        1668,  975, 1939,  926, 5719, 3103, 7529, 1658,  989, 4575, 6374, 2204,\n",
      "         846, 6791, 1113, 1484,   85, 1119, 6214, 2326, 2324, 1475, 1576, 1447,\n",
      "        2972, 4472, 4328, 5542, 5857, 6622, 7151,  344, 3350, 2906, 4932, 6450,\n",
      "        6913, 2129, 3222, 6142, 6347,  505, 7200, 5089, 5472, 6199, 2919, 5373,\n",
      "         611, 6291, 7398, 7511, 3163, 2448,  369, 7394, 4997, 5045,  449,  692,\n",
      "        1644, 1507, 7475, 5438, 7028, 3835, 2513, 3833, 2547, 4604,  566, 4545,\n",
      "        4096, 6008,  810, 7598, 2618, 4435, 5765,  610, 7461, 4955, 2425, 2009,\n",
      "        5044, 1737, 5650, 4257, 3159, 5898, 3377, 5708, 2831, 1723, 7116, 7456,\n",
      "        5507,  561, 6828, 2427,  101, 4653, 2866, 6300, 7257, 3220, 2852, 6524,\n",
      "        5396, 5381, 6205, 4838, 3472, 1054, 7562, 5808, 6432, 5506, 7089, 6865,\n",
      "        1874,  941, 1422, 4442, 5793, 2601, 4000, 4954, 2954, 1238, 1394, 7240,\n",
      "        5805, 5669, 6725, 7586, 1459, 4219, 1096, 2557, 4401, 3896, 4698, 6651,\n",
      "        4912, 1990, 7497, 1244, 7462, 3500, 3141, 7627, 5299, 5496, 4031, 2064,\n",
      "        1269, 1418, 3399, 6365, 5270, 1212, 5206, 1177, 6194, 5694, 1109, 2870,\n",
      "        6175, 2841, 5121, 4487, 5392, 3628, 3528, 7487, 4847, 4603, 6666, 4371,\n",
      "        4325, 2018, 2888,  696])\n",
      "tensor([7404, 7260, 2538, 3191, 2823, 3978, 7185, 4772, 7421, 6871, 5298, 4787,\n",
      "        1082, 6208, 7077, 3823, 1438,   71, 5213,  493, 5287, 5853, 2186, 1496,\n",
      "        7453, 3241, 1379, 3947, 5829, 2276, 2455, 2224, 7772, 5255, 1370, 1209,\n",
      "        5210, 7428, 6412, 3503, 3060, 6351, 3993, 2270, 2739, 4166, 2842, 1211,\n",
      "        1009, 4728, 2996, 2735,  391,  909, 4128, 1567,  197, 3895, 4823, 3782,\n",
      "        4338, 1397, 3560, 7225,  308, 4469,   36, 1714, 6822,  289, 5936, 7631,\n",
      "        2418, 5141, 5952, 3475, 3258, 4072, 4704, 7218, 5323, 4784, 1371, 6033,\n",
      "        4361,  185,  132, 3760, 4009, 4828, 7382, 7681, 4327,  565, 4521, 2575,\n",
      "         278, 3685, 6716, 2579, 6732, 2015, 6555,  458, 3139, 7026, 1345,  400,\n",
      "        5672, 4245, 4520, 5441, 4023, 1384, 6071, 4664, 7597, 7161, 1987, 7193,\n",
      "        1111, 6795, 6422, 2421, 6977, 2525, 5409, 1136,  532, 2280,  299, 7301,\n",
      "        4468, 3118, 7519, 6790, 5984, 7523, 3867, 2872, 5459, 3562, 2882, 7386,\n",
      "        4137, 2777, 7364, 2694, 2472, 4018, 2984, 6516, 3345,  623, 5110, 4687,\n",
      "        6264, 2491, 4446, 1421, 2059, 7496, 7585,  144, 3370, 7405, 1148,  165,\n",
      "        6552, 5689,  367, 6274, 2259, 2903, 6343, 5847, 2578, 5794,  653, 5065,\n",
      "        5825, 5833, 1464, 4983, 5571, 7327, 4837, 3028, 1360,  783, 7718, 1693,\n",
      "        3909, 1873, 4688, 4859, 2470, 5946, 1735, 7143, 1077, 5982, 3545, 7374,\n",
      "        1999, 7350, 1165, 3659, 7168, 7517, 6926, 5852, 3943, 1144, 5182, 3363,\n",
      "        6288, 7601, 4470, 7031, 1542, 3295, 5314, 2909, 3116, 7047, 5349, 7014,\n",
      "         634, 5112, 1014, 2654, 5008, 5086, 1757, 1311, 6293, 4352, 2783, 7105,\n",
      "        1040, 1966,  409, 4874, 4466, 5699, 6652, 7146, 4456, 4979, 6392,  128,\n",
      "        5115, 3882, 2153, 3441])\n",
      "tensor([6726, 5064, 2942, 6891, 6459,  873,  624,  759, 5588, 2492,  472, 6420,\n",
      "        3544, 5241, 5451, 6452,  725, 1015, 7693, 6132, 2720, 2241,  246, 4382,\n",
      "        7559, 5097,  999, 2282, 5570, 6874, 3233, 2536, 5914, 1517, 4646, 2709,\n",
      "         879,  748,  276, 1728, 5714, 4537, 4850, 6708, 6888, 1928, 6273, 5179,\n",
      "        6285, 4910, 6234, 5746, 1356, 6690, 5834, 2377, 4118, 5411, 6406, 2834,\n",
      "        1684, 3651, 6836, 3768,  531, 7628, 7188, 7349, 3426, 3824, 1781, 2140,\n",
      "        1004, 6820,  898, 5000, 5644, 4785, 1062,   53, 3277, 5832, 2454, 5286,\n",
      "        3007,  508, 1158, 4714, 4388, 1860, 5361, 5136, 4821, 4324, 2805, 4481,\n",
      "         717, 3510, 5967, 1580, 7781,  253, 3952,  440, 3889, 3199, 6375,  798,\n",
      "        5680, 5167, 4434, 5403,  252, 4033, 5639, 1387, 6560, 6927,  754,  523,\n",
      "        1705, 3497, 5992, 6108,  946, 6010, 1791, 1696, 5471, 6146, 2218,  558,\n",
      "        2057, 1549, 1521, 6002, 7790, 2545, 2387, 1579,   73, 2540, 1267, 4940,\n",
      "        3537,  528, 4186,  667, 2411, 2297, 3053, 7064, 6477, 3691,  782, 1245,\n",
      "        4776, 4110, 3502, 1989, 1163, 4084, 2531, 4742, 5007,  488,  721, 4198,\n",
      "        3343, 2398, 5256, 4650, 5836, 5559, 2787, 6580, 4514, 3783, 5677, 7606,\n",
      "        2249, 1470, 3703, 1116, 7203, 2123,  164, 2362, 4140, 1649, 4889, 1754,\n",
      "        3098, 5475, 2907, 1411, 3705, 7226,  186,  550, 7548, 1154, 7308, 6018,\n",
      "        6968,  230, 5184, 2382, 5022, 4866, 5284, 1331,  548,  652, 5659, 6860,\n",
      "         923, 7175, 4393,  587, 1335, 5841, 5231,   49, 7222, 1502, 4177, 4827,\n",
      "        4689, 3766, 2619, 5216, 1328, 5217, 2242, 7198, 6211, 3992, 6724, 3529,\n",
      "        7262, 5436, 2747, 4808, 5586, 4022, 6868, 1615, 6996, 3128, 2152, 3814,\n",
      "        3801, 4090, 4358, 2822])\n",
      "tensor([ 524, 7142, 7663, 3504, 3281, 2778, 5173, 6066, 2061,  947, 1153, 5891,\n",
      "        4410,  635, 1235, 6093, 7184, 4001, 7094, 1691,  155, 1833, 6895, 6263,\n",
      "        6507, 3442, 4986, 2090, 4551, 4782, 1532, 6202, 1008, 3071, 4397, 5722,\n",
      "        7435, 3888, 1732, 7479, 4310, 3592,  777, 2235,  464, 4011, 1669, 1351,\n",
      "         645, 4060, 6280, 7508, 3689, 6495, 2926, 2037, 6150, 7048, 6335, 5875,\n",
      "        4538, 5309, 2277, 1820, 6039, 2464, 3918, 6672, 2451,  993, 2653, 1501,\n",
      "        1221, 5684, 1372,  298, 5248, 3003, 5662,  301, 1601,  983, 5923,  608,\n",
      "        5520, 3866, 3850, 4759, 5830, 3113, 2967, 1215, 6147, 3102, 2063, 3923,\n",
      "        1539, 7708, 6178,  549, 7268, 7702, 3763, 2031, 4164, 7093, 1812,  148,\n",
      "        7018, 4007, 1050, 2815, 1868, 3638, 1835, 6518, 3737, 5554, 5404, 1023,\n",
      "        1557, 3876, 5212, 4792,  354, 6352, 6224, 2948, 4454, 3573, 1409,  673,\n",
      "        2896, 4024, 3852, 2684, 4690, 1189, 5345, 4901, 2189, 6531, 6959, 5964,\n",
      "        6858, 6906, 1162, 3758,  892, 5693, 1797, 7471,  161, 1142, 5080, 5523,\n",
      "        2800, 5653, 4780, 3264, 4692, 5181, 7639, 4624, 2717, 4396, 3485, 1525,\n",
      "        4566, 3586, 2748, 2559, 5389, 2693, 1172, 3535, 5424,  522,  525,  932,\n",
      "        6698, 1434, 6068, 2714, 3518,  495, 3388,  277, 6308, 6389, 4228,  614,\n",
      "         889, 7172, 7191, 3508, 5638, 5811, 7583, 4894, 6196, 7544, 1168,  117,\n",
      "        1081, 6393, 5786, 7302,  361, 3546, 7733, 3267,  924, 7486, 2526, 1031,\n",
      "         372, 6950, 2346, 2628, 7109, 5390, 4871, 4595, 6849, 1947, 1025, 6669,\n",
      "        2992, 6372, 1340,  829, 7571, 3359, 2956, 2511, 7137, 7728, 4565, 1604,\n",
      "        5534, 6125, 3188, 2555, 5877, 1862,  389, 5563, 6877, 4449, 6972, 5134,\n",
      "        1260, 3364, 1859,  577])\n",
      "tensor([2020, 4524, 3612, 2209, 6773, 5480,  792, 1698,  790, 5933,  331, 2181,\n",
      "        1170, 1778, 1359,  649, 5470, 7732, 5785, 5203, 6105, 6945, 1932, 3738,\n",
      "         243,  326, 7235, 1770, 2859, 2683, 2686, 5385,  656,  240, 4182, 3020,\n",
      "        7157, 4713, 1138, 4317, 5940,   42, 2155, 7246, 4923, 1596,  473, 3216,\n",
      "         397, 3957, 1617, 1357, 7070, 4921, 3559, 7611, 7657, 7568, 7332, 5598,\n",
      "        5686, 3998, 2611,  159, 1888, 3633, 3726, 3011, 4300, 5774, 2676, 2603,\n",
      "        4764, 5032,   47, 1128, 5737, 6258, 4489, 7303, 3403,  272, 1744, 1614,\n",
      "         201, 4546, 3616, 3269, 2914, 6673, 6884, 6633, 5431, 7164, 6187, 3160,\n",
      "        7079, 4492, 1934, 7174, 2974, 5635, 5422, 3490, 6036, 7672,  642, 6970,\n",
      "        3151, 4262, 3536, 3195, 4279, 7791,  895, 5761, 3186, 3285,   46, 7045,\n",
      "        2008, 5796, 1738, 3049, 1000, 1430,  988,  500, 1747, 5863, 2045, 1179,\n",
      "         225, 6562, 1908,  474, 5168, 6909,  860, 7196, 1117,  850, 6321,  533,\n",
      "         597, 5399,  791, 6168, 6944, 3912, 4607, 7770, 3520, 6885, 1540, 5977,\n",
      "        7625, 1594, 5388,   58, 6227, 5249, 1584, 5276, 7259, 4350, 3858, 4348,\n",
      "        7393,  254, 2106, 6206, 2395, 5020, 1182, 6195, 3661, 4103, 4919, 6778,\n",
      "        7560, 2911, 6714, 4305, 4441, 2100, 1848, 7589, 5149, 3284, 5126, 4409,\n",
      "         218, 6026, 2310, 4951, 1810, 5028, 3857, 6386, 3946, 3274, 2813, 1083,\n",
      "        1425, 2766, 3639,  665, 1333, 2899, 5484, 4258, 3551, 6015, 7097, 2921,\n",
      "        1703, 2875,  854, 5410, 5240, 7543,  970, 2065,  802, 3582, 3132, 6396,\n",
      "        7410,  355, 1490, 1319, 4596, 5026, 7100, 4360, 4275, 3353, 1630, 4815,\n",
      "        3792, 1713, 1094, 3406, 5612, 5070, 1543, 6999, 3827, 4788, 6281, 1606,\n",
      "        1910, 6165, 4743, 6038])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1283, 4165, 4086, 7038, 4394, 7769, 4579, 4411, 3094, 3647, 1711, 7343,\n",
      "        6114,  386, 4062, 7679, 6042, 6296, 1415, 7773, 1819, 5925, 2250, 1013,\n",
      "        1839, 3831, 6304, 3389, 3423, 6463, 7451, 1632, 3478, 5605, 4286, 3507,\n",
      "        1639, 7595, 5469, 4937, 4249, 5296, 6956, 6418,  616, 3437, 2503, 5636,\n",
      "        2331, 1407, 7369, 5336,  477, 2265, 2928, 2320, 2590, 5503,  162,  310,\n",
      "        1573, 6770, 7636, 3927, 2049, 3033, 2758, 2243, 5870, 4391,  922, 1194,\n",
      "        6035,  822, 7431, 6951, 4905, 4392, 4766, 5368, 7502, 3083,  503, 3208,\n",
      "        6162,  621, 6932, 1491, 1208,  950, 6481,  481, 4749, 6197, 7758, 2537,\n",
      "        5510, 3349, 3580,  136, 6696,  388, 6875, 5046, 3591, 2916, 5574, 3262,\n",
      "        1787, 3929, 5160, 7563, 6190, 6631, 6176, 5111, 5701, 3773, 3287, 2730,\n",
      "        1029, 2616, 1750, 1969, 7095, 7276, 3629,  681, 6931,   56, 6054, 7329,\n",
      "        5358, 7132, 1006,  765,  489, 7025,  478, 4097, 4085, 5263, 5300, 7575,\n",
      "        5461, 1310, 4231, 3611, 6667, 7677, 7163, 1959,  934, 5178, 2655,  793,\n",
      "        1151, 2185, 6686, 6359, 1133, 2632, 5477,  598, 7689, 4754, 2085, 5061,\n",
      "        4882, 6550, 3319, 4952, 6863, 6754, 1147, 5879, 3945, 7051, 5307, 1938,\n",
      "        2422, 7592,  317, 1090, 2247, 2428, 4429, 5768, 7621, 1458, 5386, 1783,\n",
      "        4747, 2476, 3744, 3893, 3469, 2036, 3924, 5462, 6655, 6510, 7700, 1760,\n",
      "         590, 3499, 3718, 4982, 1891,  212, 4266, 5748, 6181, 6180, 4805, 4389,\n",
      "        5186,  178, 3055, 2452, 2322, 2348,  683, 2837, 6473, 2771, 2117,  964,\n",
      "        2443, 7290, 1587, 3074, 2573, 7244, 6104, 2341, 7527, 5164, 5966, 2093,\n",
      "        5177, 1806,  479,  509, 4957,  270,  736, 6528, 2532, 5804,  336, 3494,\n",
      "        4202, 6099, 3903, 2139])\n",
      "tensor([1605, 4878,  462, 7121, 1611, 1585, 2012, 2889, 4846, 6905, 1053, 5050,\n",
      "          27, 6303, 6541, 6648, 5666, 1059, 3296,  720,  556, 6737, 7643, 3142,\n",
      "        2858,   63, 7584,  580, 3956, 2274, 5904,  705, 4500, 3569, 4629, 6120,\n",
      "        4803, 2431,  399, 5015, 2496,  237, 1936, 6356, 3961, 6677, 1741, 6248,\n",
      "         622, 1755, 1318, 6081, 1416, 1782, 3105,  981, 2487, 6839, 2004, 5394,\n",
      "        1323, 4293, 1559, 7685, 7574, 5578, 5569, 1481, 1663, 6975, 3155,  828,\n",
      "        1591, 1667,  430,  729, 4685, 7118, 2669, 6262, 4916,  552, 7326, 3712,\n",
      "         562, 1393, 3093, 3575,  444, 4161, 1485, 3263,  853, 2605, 7645, 1386,\n",
      "        3342,  393,  115, 3798, 1201, 1046, 5290, 1239, 6943,  808, 2881, 7668,\n",
      "        4854, 5900,   87, 6294, 7401, 3412, 1214, 2835, 1763,   65,  762, 1308,\n",
      "        3136, 7046, 2291, 4013, 4490, 7154, 2231, 2546, 4998, 6377, 1404, 1563,\n",
      "        4387,   32, 4156, 7392, 6957, 3433, 2519, 2750, 5439, 3047,  328, 1506,\n",
      "        3424, 3119, 1185, 4287, 5551, 2180, 4727, 7495, 2583, 3236, 3225, 6485,\n",
      "        7498, 3333,  709, 5264, 2086,  796, 4273, 4055, 3448, 3892, 7488, 1923,\n",
      "        2364, 7578, 3058, 1570, 7372, 3076, 1226, 7006, 3594, 2079,  563, 6387,\n",
      "        2949, 4711,  239, 5723,  927, 4933, 4448, 2659, 2706, 5842, 6738, 6360,\n",
      "        6729, 7425, 5995, 1037, 2449, 2523, 6548, 4268, 2143, 2433, 5616, 6405,\n",
      "        5654, 4277, 7156, 6100,  605, 6683, 1355,   86, 6564, 3910, 6853, 3204,\n",
      "        6685, 1256, 5333,  625, 5230, 2335, 5100, 7735, 1704, 2306, 4381,  158,\n",
      "        2871, 5989, 3762, 3246, 1060, 5095, 2371, 7181,  834, 2494, 5579, 4834,\n",
      "        2966, 5881,  309, 5670, 4189, 7058, 3949, 7390, 3221, 1385, 2342,  998,\n",
      "        5234, 6846,  427, 5372])\n",
      "tensor([3070, 3405, 2071, 6275,  351, 1828, 2453, 6917, 7688, 3601, 3326,    3,\n",
      "         931,  206, 7753, 5886, 3493, 6156, 7170,  300,  483, 2109, 4600, 2779,\n",
      "        7238, 7216, 6078,  958, 1762,  203,  211, 7698, 1213, 2840, 1121, 5812,\n",
      "        6390,  106, 1364, 4408, 6699, 4992, 5458, 4132, 4512,  607, 5189, 2719,\n",
      "        4985, 5543,  250, 1640, 5335, 6282, 7590, 3176, 3316, 5324, 4734, 5072,\n",
      "        1914, 5572, 4705, 6228, 1306, 2191, 5822, 3593, 6902,  147,  269,  435,\n",
      "        1776, 4965, 4820, 2832, 2475, 2370, 1724, 4160, 2053, 3664, 4314,  595,\n",
      "        6060, 6505, 3855, 2986, 2506, 4200, 3429, 7534, 5899, 6848, 4171, 5129,\n",
      "        7323, 7237,  633, 2558, 3714, 3977, 5626, 3356,  744, 1365,  915, 6045,\n",
      "        5716, 2213, 6240, 3200,  867, 4869,  944, 5683,  210, 2199, 5069,  669,\n",
      "        3279,  465, 3149, 7640, 5363, 1673, 4562, 2751, 6201, 2489, 5741, 4012,\n",
      "        6584, 5663, 7065, 3989, 5407, 4634, 4506, 7106, 7101, 6077, 7605, 3907,\n",
      "        7476, 6989, 5968, 5266, 4635, 7715,  789,  384, 2212, 4722, 5433, 5607,\n",
      "        3973, 4700, 3804, 2217, 4276, 7351,  350, 1906, 1330, 1440, 1188, 7278,\n",
      "        7365, 7494, 3812, 5132, 7415, 5009, 1962, 1646, 2315, 4155,  812, 1344,\n",
      "        1288, 5005, 5860, 3030, 6484,  418, 1917, 6818, 7474, 4972, 7740, 4351,\n",
      "        2819, 1298,  583, 6080,  586,  722, 7055, 2067, 4888, 5498, 6650, 6237,\n",
      "        7177, 1636, 5756, 6009, 7366, 3362,  514, 7480, 2629, 4656, 1683, 2044,\n",
      "        5437, 1702, 2745, 4436, 5039, 5896, 2378, 6468,  160, 4119, 2168, 5482,\n",
      "         780, 2657, 4323, 2110, 4384, 1740, 1715, 1581, 6798, 3340, 3444, 6011,\n",
      "        4127, 4677, 5124, 7293, 4188, 7674, 1756, 4144, 4294, 5854,  679, 7054,\n",
      "        1039, 6306, 2028, 6763])\n",
      "tensor([6338, 1716, 5260, 1879, 7608, 2430, 1100, 7150, 7744, 2757, 5180, 5878,\n",
      "        1677, 4621, 5973, 4467, 3245, 6640, 2167,  213, 1026, 4924, 6665, 6491,\n",
      "        2808,  894, 2035, 4699, 2759, 1337, 4240, 6186,  374, 1998, 3942,  626,\n",
      "        1700, 3443, 1949, 1336, 3930, 4665, 7310, 5445, 4729, 1125, 1569, 5113,\n",
      "          12,  216, 6355, 6160, 1354, 3649, 3077, 2271, 7424, 1717, 7300, 1895,\n",
      "        3807, 7149, 7220, 7128, 1389, 5623, 4732, 5577, 3174, 4094,  660, 5295,\n",
      "        5532, 4187, 7136,  740, 6581, 7190,  676,  813, 5553, 1414, 6986, 4761,\n",
      "        2593, 5985, 1950, 5703, 6122, 5017,  770, 3092, 4694, 7489, 5034, 2447,\n",
      "        1236, 5079, 6082, 7445, 2641, 5171, 5391, 5297, 3775, 1277, 6222, 3192,\n",
      "        4774, 5641, 2374, 4640, 2664, 5687,  394, 1251, 4845,  196, 5342, 2187,\n",
      "        7504, 1314, 7063, 3556, 4501, 7397, 7760, 5071, 6949, 1980, 4740, 3090,\n",
      "         851, 5429, 4104, 4354,  208, 3022, 5937, 5976, 2397, 4230, 7418, 7509,\n",
      "        2400, 3108, 4800, 1141,  180, 1672, 4247, 1034, 6962, 2830,  390, 2133,\n",
      "        5618, 6681, 4559, 3799, 5125, 1816,  153, 3425,  996, 6784, 7214, 1304,\n",
      "         594,  784, 2296, 6654, 3899, 7746, 1295, 2997, 5401, 2839,  404, 5099,\n",
      "        3460,   69,  619,  124, 4914, 3897, 6489, 7786, 4552, 2030, 6236, 5776,\n",
      "        2562, 7267, 5803, 5705, 4439, 2510, 6785, 2944, 1527, 4404, 1674, 4443,\n",
      "        3378, 2929, 5485,  949, 5365, 1858, 1730, 3754, 3891, 6991, 4651, 3609,\n",
      "          61, 5894, 2041, 7762, 2797, 1091, 6170, 5518, 5917, 3178,  547, 2551,\n",
      "         363, 5867, 3462, 4616, 4170, 6517, 7204, 7245, 3039, 2990, 1875, 2383,\n",
      "        5953, 4971,  103, 1105, 7120, 2968, 7376, 2413, 1058, 7243, 1358, 1854,\n",
      "        5712,   14, 2208, 2477])\n",
      "tensor([7111,  637, 1098, 2587, 3189, 5544, 1011, 5225, 2052, 2704, 5384, 5105,\n",
      "        5759, 3864, 5619, 5754, 1726, 1366, 6063, 2024, 1398, 5538, 5215, 6331,\n",
      "        1265, 1049, 6265, 1253, 7071, 4459, 4839, 2013, 5713, 6221,  431, 5797,\n",
      "        6632,  176, 4473, 5710, 6637, 2769,  285, 7530,  316, 3298, 3541, 4497,\n",
      "        1892, 6880, 2313, 1456,  112,  852, 2419, 1396, 6128, 6397, 3623,  745,\n",
      "        2698, 5432, 2795, 3654, 1070, 4885,  320, 2682, 3542, 3739, 4121, 1731,\n",
      "        5698, 1774, 1316, 6709,  866, 5986, 2169, 5902, 4993, 5356,  858, 2951,\n",
      "        5153,  280, 5826, 1303, 5751, 1462, 5408, 7615, 3610, 2742, 3144, 4075,\n",
      "        3249, 4567, 7169, 1749, 3145, 4794, 1807,  778, 3381, 7764, 5864, 3464,\n",
      "        6535, 2725, 2445,  445, 7752, 3122,  564, 4016, 2524, 2144, 7649,  593,\n",
      "        3243, 5040, 3291, 6225, 6423, 4344,  832, 2625, 4793, 2001, 3312, 4872,\n",
      "        4301, 5971, 5924, 2474, 3822, 2600, 1056, 4632, 2309, 5791, 2488, 1850,\n",
      "         373, 3365, 3681,  312, 2177, 5594, 1156,  187,  821, 7288, 4330, 2308,\n",
      "        1769, 4091, 6029, 2833, 7035, 7632, 5493, 1012, 2416, 5353, 4308,  573,\n",
      "        1092, 3184, 6908, 2518, 1399, 7282,  490, 5499, 4471, 5075, 3770, 6588,\n",
      "        6983, 4667, 2275, 5660, 3143, 5114, 2658, 2178, 3320, 4122, 3935, 6043,\n",
      "        4077, 5226, 3387, 1592, 7072, 5193, 4913, 2084, 6833, 4211, 7217, 1743,\n",
      "        3009, 1088, 1483, 1341, 3898, 5497, 1687, 1429, 3124, 3707, 3130, 2981,\n",
      "        6001, 3630, 1103, 5328, 5378, 7165, 1751, 6821, 6193, 3218,   92, 2588,\n",
      "        3656,  172, 6576, 1953, 2438, 6590, 2672, 4903, 7468, 6720,  864, 3454,\n",
      "        5161, 6474, 6734, 1246, 3745, 6216, 6094,  370, 1834, 7417, 4015, 3811,\n",
      "        5744, 4280, 6102, 7296])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7377, 7355, 4373, 1518, 5890, 7264, 4946, 5568, 2267, 7408, 5690, 4810,\n",
      "        5861,  827, 4590, 1974, 3468,  226,  116, 1190,  174, 1075,   35, 7531,\n",
      "        1815, 2442, 6859,  156,  887, 2434, 1832, 1964, 3065, 3467, 2624, 1052,\n",
      "        5351, 1753, 4235, 5911, 4887, 3395,  841,  526, 7361, 5664, 6357, 3843,\n",
      "        5880, 7716, 6678, 3932, 1493, 4190, 4627, 2043, 7205,  125, 2789, 1911,\n",
      "        5038, 6106, 4931, 7152, 7472, 2435, 1775, 5688, 2740, 2668, 5329, 5452,\n",
      "        4337,  978, 3110,  487, 6596, 7102, 1248, 2351, 6521, 4527, 3568, 7254,\n",
      "        7126, 1325, 2930, 6032, 3390, 5685, 2156, 4525, 3075, 5613, 3566,    7,\n",
      "        6429,   21,  763, 5138,  974, 6739, 3129, 2193, 6287, 3849, 2051, 3207,\n",
      "        2391, 4407, 2245, 2480,  417, 3067, 2598, 6438, 6636, 5948,  282, 1978,\n",
      "        1413, 6232, 5362, 6769, 4836, 4462,  422, 1278,  723, 3483, 7346, 6606,\n",
      "        3848, 6115, 4911, 4814, 2564, 2613, 6527, 6509, 4424, 5315, 7335, 3856,\n",
      "        3825, 5903, 6328, 7331, 2728, 5919, 7091, 7255,   70, 7440, 4723, 6380,\n",
      "        3625, 4478, 7019, 1324,  236, 2959, 4252, 2173,  904, 4049, 7030, 1808,\n",
      "        7594, 6152, 4178, 4832, 7352,  323, 4602, 4864, 3648, 3339,    4, 3617,\n",
      "         584, 5043, 6433, 2614, 1851, 6159, 4856, 4444, 1670, 3035, 2607, 3563,\n",
      "        4047, 3862, 6247, 6252, 4336, 5632, 3585,  771, 1274, 5707,  776, 2284,\n",
      "        4020, 6118, 3905, 5799, 7588, 6427, 7208, 6447, 2088,  542, 5609, 2971,\n",
      "        4431, 7467, 1508, 6664, 6793, 3154,  968, 3428, 1368, 5271, 2160,  955,\n",
      "        3256, 6623, 2606, 6954, 6174, 3985, 3237, 1829, 6384, 3619, 1742, 1764,\n",
      "        3868, 4963, 1784, 2373, 1907, 2115, 3774, 6255, 5730, 7630,  727,  150,\n",
      "         876, 2851, 6583, 7542])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8d905b74a692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mx_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_single_jet_augs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0mx_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshift_phi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mx_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshift_eta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MJetCLR/scripts/modules/jet_augs.py\u001b[0m in \u001b[0;36mapply_single_jet_augs\u001b[0;34m(events, njets, center, rot, trs, dis, col, trsw, ptst, ptcm)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0msubjet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollinear_fill_jets\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msubjet\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0msubjet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollinear_fill_jets\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msubjet\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m#plot_jets_phase_plane(subjet[l], subjet[l], 2, xlims=(-3,3), ylims=(-3,3))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MJetCLR/scripts/modules/jet_augs.py\u001b[0m in \u001b[0;36mcollinear_fill_jets\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mbatchb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mnc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mnzs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mnzs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mnzs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_transformer = True\n",
    "\n",
    "\n",
    "train_num_only = False\n",
    "train_den_only = False\n",
    "\n",
    "n_constits_max = grading\n",
    "\n",
    "if run_transformer:\n",
    "    \n",
    "    # THE TRAINING LOOP\n",
    "\n",
    "    # initialise lists for storing training stats, validation loss\n",
    "    losses_clr_num_jets = {i:[] for i in range(grading,n_constits_max+grading,grading)}\n",
    "    losses_clr_numer_num_jets = {i:[] for i in range(grading,n_constits_max+grading,grading)}\n",
    "    losses_clr_denom_num_jets = {i:[] for i in range(grading,n_constits_max+grading,grading)}\n",
    "    loss_validation_num_jets = {i:[[],[]] for i in range(grading,n_constits_max+grading,grading)} #epoch, loss\n",
    "    lct_auc_num_jets = {i:[[],[],[],[]] for i in range(grading,n_constits_max+grading,grading)} #epoch, auc (pt, eta, phi)\n",
    "\n",
    "\n",
    "    n_epochs = 20\n",
    "    loss_check_epoch = 2\n",
    "    verbal_epoch = 2\n",
    "\n",
    "    mean_consts_post_split = []\n",
    "\n",
    "\n",
    "\n",
    "    for constit_num in range(grading,n_constits_max+grading,grading):\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        print( \"starting training loop, running for \" + str( n_epochs ) + \" epochs\" + \" with \" + str(constit_num) + \" constituents\" \n",
    "              , flush=True)\n",
    "        print(\"Training data shape:\",clr_train.shape)\n",
    "        print(\"Validation data shape:\",clr_val.shape)\n",
    "        print( \"---\", flush=True )\n",
    "\n",
    "        # re-batch the data on each epoch\n",
    "        for epoch in range( n_epochs + 1 ):\n",
    "\n",
    "            # get batch_size number of indices\n",
    "            indices_list = torch.split( torch.randperm( clr_train.shape[0] ), batch_size )\n",
    "\n",
    "            # initialise lists to store batch stats\n",
    "            losses_clr_e = []\n",
    "            losses_clr_numer_e = []\n",
    "            losses_clr_denom_e = []\n",
    "\n",
    "            # the inner loop goes through the dataset batch by batch\n",
    "            # augmentations of the jets are done on the fly\n",
    "            for i, indices in enumerate( indices_list ): # random jets from the dataset\n",
    "                net.optimizer.zero_grad()\n",
    "                \"\"\"\n",
    "                TRANSFORMATIONS AND DATA PREPARATION\n",
    "                \"\"\"\n",
    "                x_i = clr_train[indices,:,:]\n",
    "                \n",
    "                x_i, x_j = apply_single_jet_augs(x_i, 2, center, rot, trs, dis, col)\n",
    "                x_j = shift_phi(x_j)\n",
    "                x_j = shift_eta(x_j)\n",
    "\n",
    "                # rescaling pT\n",
    "                max_pt = np.max(x_i[:,0,:])\n",
    "                pt_rescale_denom  = max_pt/ 10.\n",
    "                x_i = rescale_pts( x_i, pt_rescale_denom )\n",
    "                x_j = rescale_pts( x_j, pt_rescale_denom )\n",
    "\n",
    "\n",
    "                mean_consts_post_split.append(np.mean(get_num_constits(x_j)))\n",
    "\n",
    "                x_i = torch.Tensor( x_i ).transpose(1,2).to( device ) # shape (batchsize, 2, 3)\n",
    "                x_j = torch.Tensor( x_j ).transpose(1,2).to( device )\n",
    "                z_i = net( x_i, use_mask=mask, use_continuous_mask=cmask ) # shape (batchsize, output_dim)\n",
    "                z_j = net( x_j, use_mask=mask, use_continuous_mask=cmask )\n",
    "\n",
    "                \"\"\"\n",
    "                LOSS CALCULATIONS\n",
    "                \"\"\"            \n",
    "                # compute the loss based on predictions of the net and the correct answers\n",
    "                loss = contrastive_loss( z_i, z_j, device, temperature, 1 ).to( device )\n",
    "                loss_numer, loss_denom = contrastive_loss_num_den( z_i, z_j, device, temperature , 1)\n",
    "                \n",
    "                if train_den_only:\n",
    "                    loss_denom.backward()\n",
    "                elif train_num_only:\n",
    "                    loss_numer.backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    \n",
    "                loss_numer = loss_numer.detach().cpu().numpy()\n",
    "                loss_denom = loss_denom.detach().cpu().numpy()\n",
    "                \n",
    "                    \n",
    "                net.optimizer.step()\n",
    "                net.optimizer.zero_grad()\n",
    "                losses_clr_e.append( loss.detach().cpu().numpy() )\n",
    "                losses_clr_numer_e.append(loss_numer)\n",
    "                losses_clr_denom_e.append(loss_denom)\n",
    "\n",
    "            \"\"\"\n",
    "            AVERAGING OF LOSSES\n",
    "            \"\"\" \n",
    "            loss_clr_e = np.mean( np.array( losses_clr_e ) )\n",
    "            ## scheduler\n",
    "            scheduler.step( loss_clr_e )\n",
    "\n",
    "            # storage\n",
    "            losses_clr_num_jets[constit_num].append( loss_clr_e )\n",
    "            losses_clr_numer_num_jets[constit_num].append( np.mean( np.array( losses_clr_numer_e ) ) )\n",
    "            losses_clr_denom_num_jets[constit_num].append( np.mean( np.array( losses_clr_denom_e ) ) )\n",
    "\n",
    "            \"\"\"\n",
    "            EVERY SO OFTEN, GIVEN AN UPDATE\n",
    "            \"\"\"\n",
    "\n",
    "            if epoch % verbal_epoch == 0:\n",
    "\n",
    "\n",
    "                print( \"epoch: \" + str( epoch ) + \", loss: \" + str( round(losses_clr_num_jets[constit_num][-1], 5) ), flush=True )\n",
    "                #print( \"lr: \" + str( scheduler._last_lr ), flush=True  )\n",
    "                # summarize alignment and uniformity stats\n",
    "                print( \"numerator: \" + str( losses_clr_numer_num_jets[constit_num][-1] ) + \", denominator: \" + str( losses_clr_denom_num_jets[constit_num][-1] ), flush=True)\n",
    "                print(\"time taken up to now: \" + str(time.time()-t0))\n",
    "                print()\n",
    "\n",
    "            if epoch % loss_check_epoch == 0:\n",
    "\n",
    "                \"\"\"\n",
    "                Get the validation loss\n",
    "                \"\"\"\n",
    "                print(\"Getting the validation CLR loss...\")\n",
    "                # store the epoch\n",
    "                loss_validation_num_jets[constit_num][0].append(epoch)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval()\n",
    "\n",
    "                    # get batch_size number of indices\n",
    "                    indices_list_val = torch.split( torch.randperm( clr_val.shape[0] ), batch_size )\n",
    "                    local_val_losses = []\n",
    "\n",
    "                    for j, indices_val in enumerate( indices_list_val ):\n",
    "                        \n",
    "                        a_i = clr_val[indices_val,:,:]\n",
    "                        a_i, a_j = apply_single_jet_augs(a_i, 2, center, rot, trs, dis, col)\n",
    "                        a_j = shift_phi(a_j)\n",
    "                        a_j = shift_eta(a_j)\n",
    "\n",
    "                        # rescaling pT\n",
    "                        max_pt = np.max(a_i[:,0,:])\n",
    "                        pt_rescale_denom  = max_pt/ 10.\n",
    "                        a_i = rescale_pts( a_i, pt_rescale_denom )\n",
    "                        a_j = rescale_pts( a_j, pt_rescale_denom )\n",
    "\n",
    "\n",
    "                        a_i = torch.Tensor( a_i ).transpose(1,2).to( device ) # shape (batchsize, 2, 3)\n",
    "                        a_j = torch.Tensor( a_j ).transpose(1,2).to( device )\n",
    "                        w_i = net( a_i, use_mask=mask, use_continuous_mask=cmask ) # shape (batchsize, output_dim)\n",
    "                        w_j = net( a_j, use_mask=mask, use_continuous_mask=cmask )\n",
    "\n",
    "                        loss_val = contrastive_loss( w_i, w_j, device, temperature, 1 ).to( device )\n",
    "                        local_val_losses.append(loss_val.detach().cpu().numpy())\n",
    "\n",
    "                    loss_val_e = np.mean( np.array( local_val_losses ) )\n",
    "                    loss_validation_num_jets[constit_num][1].append(loss_val_e)\n",
    "\n",
    "                \"\"\"\n",
    "                Run a LCT for signal vs background (supervised)\n",
    "                \"\"\"\n",
    "\n",
    "                net.eval()\n",
    "                #lct_train_reps = F.normalize( net.forward_batchwise( torch.Tensor( data_test_f ).transpose(1,2), data_test_f.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu(), dim=-1  ).numpy()\n",
    "                #lct_test_reps = F.normalize( net.forward_batchwise( torch.Tensor( data_val ).transpose(1,2), data_val.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu(), dim=-1  ).numpy()\n",
    "                lct_train_reps = net.forward_batchwise( torch.Tensor( data_test_f ).transpose(1,2), data_test_f.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu().numpy()\n",
    "                lct_test_reps = net.forward_batchwise( torch.Tensor( data_val ).transpose(1,2), data_val.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu().numpy()\n",
    "\n",
    "            \n",
    "                print(\"Doing a short LCT...\")\n",
    "\n",
    "                lct_auc_num_jets[constit_num][0].append(epoch)\n",
    "                # Need to transform the data into the representation space first\n",
    "                with torch.no_grad():\n",
    "                    for trait in range(lct_train_reps.shape[1]): # going through the layers of the transformer\n",
    "                        # run the LCT\n",
    "                        reg = LinearRegression().fit(lct_train_reps[:,trait,:], labels_test_f)\n",
    "                        # make the prediction\n",
    "                        predictions = reg.predict(lct_test_reps[:,trait,:])\n",
    "                        auc = roc_auc_score(labels_val, predictions)\n",
    "                        lct_auc_num_jets[constit_num][1+trait].append(auc)\n",
    "            \n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        print( \"JETCLR TRAINING DONE, time taken: \" + str( np.round( t1-t0, 2 ) ), flush=True)\n",
    "\n",
    "\n",
    "        # save out results\n",
    "        print( \"saving out data/results\", flush=True)\n",
    "        np.save( expt_dir+\"clr_losses_train_\"+str(constit_num)+\".npy\", losses_clr_num_jets[constit_num] )\n",
    "        np.save( expt_dir+\"clr_numer_loss_train_\"+str(constit_num)+\".npy\", losses_clr_numer_num_jets[constit_num] )\n",
    "        np.save( expt_dir+\"clr_denom_loss_train_\"+str(constit_num)+\".npy\", losses_clr_denom_num_jets[constit_num] )\n",
    "        np.save( expt_dir+\"clr_losses_val_\"+str(constit_num)+\".npy\", loss_validation_num_jets[constit_num] )\n",
    "        np.save( expt_dir+\"lct_auc_\"+str(constit_num)+\".npy\", lct_auc_num_jets[constit_num] )\n",
    "\n",
    "        # save out final trained model\n",
    "        print( \"saving out final jetCLR model\", flush=True )\n",
    "        torch.save(net.state_dict(), expt_dir+\"final_model_\"+str(constit_num)+\".pt\")\n",
    "        print()\n",
    "\n",
    "        print(\"Avg # constits:\", np.mean(mean_consts_post_split))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91182c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the training contrastive losses\n",
    "\"\"\"\n",
    "plot_clr_losses = []\n",
    "\n",
    "plot_clr_losses.append((range(len(losses_clr_num_jets[constit_num])),\n",
    "                       losses_clr_num_jets[constit_num], \"CLR loss, \"+str(constit_num) + \" constits\"))\n",
    "plot_clr_losses.append((loss_validation_num_jets[constit_num][0],\n",
    "                       loss_validation_num_jets[constit_num][1],\"Val loss, \"+str(constit_num) + \" constits\"))\n",
    "plot_losses(plot_clr_losses, \"Contrastive losses, training\", True)  \n",
    "    \n",
    "\"\"\"\n",
    "Plot the LC + NN AUC\n",
    "\"\"\"\n",
    "\n",
    "plot_LCT_stats = []\n",
    "plot_LCT_stats.append((lct_auc_num_jets[constit_num][0], lct_auc_num_jets[constit_num][1],\n",
    "                         \"LC transformer, \"+str(constit_num) + \" constits\"))\n",
    "plot_LCT_stats.append((lct_auc_num_jets[constit_num][0], lct_auc_num_jets[constit_num][2],\n",
    "                        \"LC hidden layer, \"+str(constit_num) + \" constits\"))\n",
    "plot_LCT_stats.append((lct_auc_num_jets[constit_num][0], lct_auc_num_jets[constit_num][3],\n",
    "                        \"LC output layer, \"+str(constit_num) + \" constits\"))\n",
    "\n",
    "plot_losses(plot_LCT_stats, \"ROC Area\", False)  \n",
    "\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Plot the training contrastive losses num + denom\n",
    "\"\"\"\n",
    "\n",
    "plot_num_val_losses = []\n",
    "\n",
    "plot_num_val_losses.append((range(len(losses_clr_numer_num_jets[constit_num])),\n",
    "                       -np.array(losses_clr_numer_num_jets[constit_num]), str(constit_num) + \" constits\"))\n",
    "plot_losses(plot_num_val_losses, \"-Alignment losses (should increase)\", True)  \n",
    "\n",
    "plot_den_val_losses = []\n",
    "\n",
    "plot_den_val_losses.append((range(len(losses_clr_denom_num_jets[constit_num])),\n",
    "                       np.array(losses_clr_denom_num_jets[constit_num]),  str(constit_num) + \" constits\"))\n",
    "plot_losses(plot_den_val_losses, \"Uniformity losses (should decrease)\", True)  \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f5e95-c2d7-42dc-8e34-9b5cf14dd5cf",
   "metadata": {},
   "source": [
    "# Run final LCT on the transformer representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc28ffb-1178-41ef-b7b0-cf22f760f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "constit_num = grading\n",
    "\n",
    "# Loading in the final transformer\n",
    "\n",
    "loaded_net = Transformer( input_dim, model_dim, output_dim, n_heads, dim_feedforward, \n",
    "                  n_layers, learning_rate_trans, n_head_layers, dropout=0.1, opt=opt )\n",
    "\n",
    "loaded_net.load_state_dict(torch.load(expt_dir+\"final_model_\"+str(constit_num)+\".pt\"))\n",
    "loaded_net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2205eac3-c69d-4b54-9acf-abf238eb51b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the final transformer on the binary classification data\n",
    "\n",
    "print(\"Loading data into net...\")\n",
    "#lct_train_reps = F.normalize( loaded_net.forward_batchwise( torch.Tensor( data_train ).transpose(1,2), data_train.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu(), dim=-1  ).numpy()\n",
    "#lct_test_reps = F.normalize( loaded_net.forward_batchwise( torch.Tensor( data_test_f ).transpose(1,2), data_test_f.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu(), dim=-1  ).numpy()\n",
    "lct_train_reps = loaded_net.forward_batchwise( torch.Tensor( data_train ).transpose(1,2), data_train.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu().numpy()\n",
    "lct_test_reps =  loaded_net.forward_batchwise( torch.Tensor( data_test_f ).transpose(1,2), data_test_f.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu().numpy()\n",
    "\n",
    "print(\"Data loaded!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1ad8f-d110-4896-b89e-e69e603adaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lct_fpt_tpr = {0:{\"fpr\":[],\"tpr\":[]},\n",
    "              1:{\"fpr\":[],\"tpr\":[]},\n",
    "              2:{\"fpr\":[],\"tpr\":[]}}\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "print(\"Doing a LCT...\")\n",
    "# Need to transform the data into the representation space first\n",
    "with torch.no_grad():\n",
    "    for trait in range(lct_train_reps.shape[1]): # going through the layers of the transformer\n",
    "        # run the LCT\n",
    "\n",
    "       \n",
    "        reg = LinearRegression().fit(lct_train_reps[:,trait,:], labels_train)\n",
    "        # make the prediction\n",
    "        predictions = reg.predict(lct_test_reps[:,trait,:])\n",
    "        fpr, tpr, _ = roc_curve(labels_test_f, predictions)\n",
    "        \n",
    "        plt.plot(tpr, 1.0/fpr, label = \"LCT\"+str(trait))\n",
    "        \n",
    "        np.save( expt_dir+\"CLR_LCT\"+str(trait)+\"_fpr_\"+str(constit_num)+\".npy\", fpr )\n",
    "        np.save( expt_dir+\"CLR_LCT\"+str(trait)+\"_tpr_\"+str(constit_num)+\".npy\", tpr )\n",
    "        \n",
    "        predicted = np.round(predictions).reshape(labels_test_f.size)\n",
    "        total = labels_test_f.size\n",
    "        correct = (predicted == labels_test_f).sum()    \n",
    "        \n",
    "        print(\"CLR_LCT\"+str(trait),\"acc:\",correct/total)\n",
    "        \n",
    "print(\"LCT data saved\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"1/(False Positive Rate)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca05eca-25ef-4c41-94bb-404aaae36882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffee9e5-937c-475d-ab53-ba53f2f6dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b948293-9a47-477f-a50a-9cb1375a1a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d393ec-4032-4883-9c2b-f59549a851d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f88324d-16ea-4802-8f72-acd1637ee254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5b18c-c754-4d6f-96e2-e6ceb3ea72c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e9bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c955666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb36fe8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21fa70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f92f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f863ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
