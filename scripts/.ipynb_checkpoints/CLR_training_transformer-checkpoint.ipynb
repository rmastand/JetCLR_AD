{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e80fafd-9a03-435d-a5eb-d6132a9012e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# load standard python modules\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# load torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# load custom modules required for jetCLR training\n",
    "from modules.jet_augs import apply_single_jet_augs, translate_jets, rotate_jets, rescale_pts, distort_jets, collinear_fill_jets, crop_jets\n",
    "from modules.jet_augs import shift_eta, shift_phi\n",
    "from modules.transformer import Transformer\n",
    "from modules.losses import contrastive_loss, align_loss, uniform_loss, contrastive_loss_num_den\n",
    "from modules.perf_eval import get_perf_stats, linear_classifier_test, plot_losses\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c7ac9c-3b98-4534-bb70-66b0aa2d4f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "ERROR: experiment already exists, don't want to overwrite it by mistake\n",
      "experiment: dijet_dim_scan_50_0_large/0512d/\n"
     ]
    }
   ],
   "source": [
    "# More parameters / computing setup\n",
    "\n",
    "# set the number of threads that pytorch will use\n",
    "torch.set_num_threads(2)\n",
    "\n",
    "exp_id = \"dijet_dim_scan_50_0_large/0512d/\"\n",
    "\n",
    "# set gpu device\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print( \"device: \" + str( device ), flush=True)\n",
    "\n",
    "# set up results directory\n",
    "base_dir = \"/global/home/users/rrmastandrea/MJetCLR/\"  # change this to your working directory\n",
    "expt_dir = base_dir + \"projects/rep_learning/experiments/\" + exp_id + \"/\"\n",
    "\n",
    "#check if experiment alreadyexists\n",
    "if os.path.isdir(expt_dir):\n",
    "    print(\"ERROR: experiment already exists, don't want to overwrite it by mistake\")\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(expt_dir)\n",
    "\n",
    "print(\"experiment: \"+str(exp_id) , flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f768aa5",
   "metadata": {},
   "source": [
    "# Load in the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5078731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/home/users/rrmastandrea/training_data/n_sig_17764_n_bkg_50000_n_nonzero_50_n_pad_0_n_jet_2/\n",
      "CLR training data shape: (16000, 3, 102)\n",
      "CLR val data shape: (4000, 3, 102)\n",
      "BC training data shape: (54211, 3, 102)\n",
      "BC training labels shape: (54211,)\n",
      "BC val data shape: (6776, 3, 102)\n",
      "BC val labels shape: (6776,)\n",
      "BC test data shape: (6776, 3, 102)\n",
      "BC test labels shape: (6776,)\n"
     ]
    }
   ],
   "source": [
    "path_to_save_dir = \"/global/home/users/rrmastandrea/training_data/\"\n",
    "\n",
    "\n",
    "#save_id_dir = \"n_sig_6557_n_bkg_8000_n_nonzero_50_n_pad_0/\"\n",
    "save_id_dir = \"n_sig_17764_n_bkg_50000_n_nonzero_50_n_pad_0_n_jet_2/\"\n",
    "\n",
    "grading = 50\n",
    "\n",
    "path_to_data = path_to_save_dir+save_id_dir\n",
    "print(path_to_data)\n",
    "\n",
    "\n",
    "clr_train = np.load(path_to_data+\"clr_train.npy\")\n",
    "clr_val = np.load(path_to_data+\"clr_val.npy\")\n",
    "data_train = np.load(path_to_data+\"data_train.npy\")\n",
    "labels_train = np.load(path_to_data+\"labels_train.npy\")\n",
    "data_val = np.load(path_to_data+\"data_val.npy\")\n",
    "labels_val = np.load(path_to_data+\"labels_val.npy\")\n",
    "data_test_f = np.load(path_to_data+\"data_test_f.npy\")\n",
    "labels_test_f = np.load(path_to_data+\"labels_test_f.npy\")\n",
    "\n",
    "# print data dimensions\n",
    "print( \"CLR training data shape: \" + str( clr_train.shape ), flush=True)\n",
    "print( \"CLR val data shape: \" + str( clr_val.shape ), flush=True)\n",
    "print( \"BC training data shape: \" + str( data_train.shape ), flush=True)\n",
    "print( \"BC training labels shape: \" + str( labels_train.shape ), flush=True)\n",
    "print( \"BC val data shape: \" + str( data_val.shape ), flush=True)\n",
    "print( \"BC val labels shape: \" + str( labels_val.shape ), flush=True)\n",
    "print( \"BC test data shape: \" + str( data_test_f.shape ), flush=True)\n",
    "print( \"BC test labels shape: \" + str( labels_test_f.shape ), flush=True)\n",
    "\n",
    "\n",
    "# Plot num constituents\n",
    "\n",
    "def get_num_constits(dataset):\n",
    "    consits_list = []\n",
    "    for collision in dataset:\n",
    "        pts = collision[0,:]\n",
    "\n",
    "        pads = np.where(pts==0)\n",
    "        consits_list.append(dataset.shape[2]-len(pads[0]))\n",
    "        \n",
    "    return consits_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e82160-2164-4206-9ea1-76cb130c0567",
   "metadata": {},
   "source": [
    "# Define the Transformer Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f66a6402-43b2-42d7-b387-a812915ccaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the transformer net\n",
    "\"\"\"\n",
    "\n",
    "# transformer hyperparams\n",
    "# input dim to the transformer -> (pt,eta,phi)\n",
    "input_dim = 3\n",
    "model_dim = 512\n",
    "output_dim = model_dim\n",
    "dim_feedforward = model_dim\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "n_head_layers = 2\n",
    "opt = \"adam\"\n",
    "\n",
    "mask= False\n",
    "cmask = True\n",
    "\n",
    "learning_rate_trans = 0.0001\n",
    "batch_size = 256\n",
    "temperature = .8\n",
    "\n",
    "# augmentations\n",
    "rot = True # rotations\n",
    "trs = True # translations\n",
    "dis = True # distortion\n",
    "col = True # collinear\n",
    "\n",
    "center = \"J1_phi_only_pi_2\"\n",
    "\n",
    "\n",
    "#rescale = 1.0\n",
    "\n",
    "net = Transformer( input_dim, model_dim, output_dim, n_heads, dim_feedforward, \n",
    "                  n_layers, learning_rate_trans, n_head_layers, dropout=0.1, opt=opt )\n",
    "\n",
    "## send network to device\n",
    "net.to( device )\n",
    "\n",
    "# define lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( net.optimizer, factor=0.2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838b294-8030-4264-9fc5-e787408bc9f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_transformer = True\n",
    "\n",
    "\n",
    "train_num_only = False\n",
    "train_den_only = False\n",
    "\n",
    "n_constits_max = grading\n",
    "\n",
    "if run_transformer:\n",
    "    \n",
    "    # THE TRAINING LOOP\n",
    "\n",
    "    # initialise lists for storing training stats, validation loss\n",
    "    losses_clr_num_jets = {i:[] for i in range(grading,n_constits_max+grading,grading)}\n",
    "    losses_clr_numer_num_jets = {i:[] for i in range(grading,n_constits_max+grading,grading)}\n",
    "    losses_clr_denom_num_jets = {i:[] for i in range(grading,n_constits_max+grading,grading)}\n",
    "    loss_validation_num_jets = {i:[[],[]] for i in range(grading,n_constits_max+grading,grading)} #epoch, loss\n",
    "    lct_auc_num_jets = {i:[[],[],[],[]] for i in range(grading,n_constits_max+grading,grading)} #epoch, auc (pt, eta, phi)\n",
    "\n",
    "\n",
    "    n_epochs = 100\n",
    "    loss_check_epoch = 5\n",
    "    verbal_epoch = 5\n",
    "\n",
    "    mean_consts_post_split = []\n",
    "\n",
    "\n",
    "\n",
    "    for constit_num in range(grading,n_constits_max+grading,grading):\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        print( \"starting training loop, running for \" + str( n_epochs ) + \" epochs\" + \" with \" + str(constit_num) + \" constituents\" \n",
    "              , flush=True)\n",
    "        print(\"Training data shape:\",clr_train.shape)\n",
    "        print(\"Validation data shape:\",clr_val.shape)\n",
    "        print( \"---\", flush=True )\n",
    "\n",
    "        # re-batch the data on each epoch\n",
    "        for epoch in range( n_epochs + 1 ):\n",
    "\n",
    "            # get batch_size number of indices\n",
    "            indices_list = torch.split( torch.randperm( clr_train.shape[0] ), batch_size )\n",
    "\n",
    "            # initialise lists to store batch stats\n",
    "            losses_clr_e = []\n",
    "            losses_clr_numer_e = []\n",
    "            losses_clr_denom_e = []\n",
    "\n",
    "            # the inner loop goes through the dataset batch by batch\n",
    "            # augmentations of the jets are done on the fly\n",
    "            for i, indices in enumerate( indices_list ): # random jets from the dataset\n",
    "                net.optimizer.zero_grad()\n",
    "                \"\"\"\n",
    "                TRANSFORMATIONS AND DATA PREPARATION\n",
    "                \"\"\"\n",
    "                x_i = clr_train[indices,:,:]\n",
    "                \n",
    "                x_i, x_j = apply_single_jet_augs(x_i, 2, center, rot, trs, dis, col)\n",
    "                x_j = shift_phi(x_j)\n",
    "                x_j = shift_eta(x_j)\n",
    "\n",
    "                # rescaling pT\n",
    "                max_pt = np.max(x_i[:,0,:])\n",
    "                pt_rescale_denom  = max_pt/ 10.\n",
    "                x_i = rescale_pts( x_i, pt_rescale_denom )\n",
    "                x_j = rescale_pts( x_j, pt_rescale_denom )\n",
    "\n",
    "\n",
    "                mean_consts_post_split.append(np.mean(get_num_constits(x_j)))\n",
    "\n",
    "                x_i = torch.Tensor( x_i ).transpose(1,2).to( device ) # shape (batchsize, 2, 3)\n",
    "                x_j = torch.Tensor( x_j ).transpose(1,2).to( device )\n",
    "                z_i = net( x_i, use_mask=mask, use_continuous_mask=cmask ) # shape (batchsize, output_dim)\n",
    "                z_j = net( x_j, use_mask=mask, use_continuous_mask=cmask )\n",
    "\n",
    "                \"\"\"\n",
    "                LOSS CALCULATIONS\n",
    "                \"\"\"            \n",
    "                # compute the loss based on predictions of the net and the correct answers\n",
    "                loss = contrastive_loss( z_i, z_j, device, temperature, 1 ).to( device )\n",
    "                loss_numer, loss_denom = contrastive_loss_num_den( z_i, z_j, device, temperature , 1)\n",
    "                \n",
    "                if train_den_only:\n",
    "                    loss_denom.backward()\n",
    "                elif train_num_only:\n",
    "                    loss_numer.backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    \n",
    "                loss_numer = loss_numer.detach().cpu().numpy()\n",
    "                loss_denom = loss_denom.detach().cpu().numpy()\n",
    "                \n",
    "                    \n",
    "                net.optimizer.step()\n",
    "                net.optimizer.zero_grad()\n",
    "                losses_clr_e.append( loss.detach().cpu().numpy() )\n",
    "                losses_clr_numer_e.append(loss_numer)\n",
    "                losses_clr_denom_e.append(loss_denom)\n",
    "\n",
    "            \"\"\"\n",
    "            AVERAGING OF LOSSES\n",
    "            \"\"\" \n",
    "            loss_clr_e = np.mean( np.array( losses_clr_e ) )\n",
    "            ## scheduler\n",
    "            scheduler.step( loss_clr_e )\n",
    "\n",
    "            # storage\n",
    "            losses_clr_num_jets[constit_num].append( loss_clr_e )\n",
    "            losses_clr_numer_num_jets[constit_num].append( np.mean( np.array( losses_clr_numer_e ) ) )\n",
    "            losses_clr_denom_num_jets[constit_num].append( np.mean( np.array( losses_clr_denom_e ) ) )\n",
    "\n",
    "            \"\"\"\n",
    "            EVERY SO OFTEN, GIVEN AN UPDATE\n",
    "            \"\"\"\n",
    "\n",
    "            if epoch % verbal_epoch == 0:\n",
    "\n",
    "\n",
    "                print( \"epoch: \" + str( epoch ) + \", loss: \" + str( round(losses_clr_num_jets[constit_num][-1], 5) ), flush=True )\n",
    "                #print( \"lr: \" + str( scheduler._last_lr ), flush=True  )\n",
    "                # summarize alignment and uniformity stats\n",
    "                print( \"numerator: \" + str( losses_clr_numer_num_jets[constit_num][-1] ) + \", denominator: \" + str( losses_clr_denom_num_jets[constit_num][-1] ), flush=True)\n",
    "                print(\"time taken up to now: \" + str(time.time()-t0))\n",
    "                print()\n",
    "\n",
    "            if epoch % loss_check_epoch == 0:\n",
    "\n",
    "                \"\"\"\n",
    "                Get the validation loss\n",
    "                \"\"\"\n",
    "                print(\"Getting the validation CLR loss...\")\n",
    "                # store the epoch\n",
    "                loss_validation_num_jets[constit_num][0].append(epoch)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval()\n",
    "\n",
    "                    # get batch_size number of indices\n",
    "                    indices_list_val = torch.split( torch.randperm( clr_val.shape[0] ), batch_size )\n",
    "                    local_val_losses = []\n",
    "\n",
    "                    for j, indices_val in enumerate( indices_list_val ):\n",
    "                        \n",
    "                        a_i = clr_val[indices_val,:,:]\n",
    "                        a_i, a_j = apply_single_jet_augs(a_i, 2, center, rot, trs, dis, col)\n",
    "                        a_j = shift_phi(a_j)\n",
    "                        a_j = shift_eta(a_j)\n",
    "\n",
    "                        # rescaling pT\n",
    "                        max_pt = np.max(a_i[:,0,:])\n",
    "                        pt_rescale_denom  = max_pt/ 10.\n",
    "                        a_i = rescale_pts( a_i, pt_rescale_denom )\n",
    "                        a_j = rescale_pts( a_j, pt_rescale_denom )\n",
    "\n",
    "\n",
    "                        a_i = torch.Tensor( a_i ).transpose(1,2).to( device ) # shape (batchsize, 2, 3)\n",
    "                        a_j = torch.Tensor( a_j ).transpose(1,2).to( device )\n",
    "                        w_i = net( a_i, use_mask=mask, use_continuous_mask=cmask ) # shape (batchsize, output_dim)\n",
    "                        w_j = net( a_j, use_mask=mask, use_continuous_mask=cmask )\n",
    "\n",
    "                        loss_val = contrastive_loss( w_i, w_j, device, temperature, 1 ).to( device )\n",
    "                        local_val_losses.append(loss_val.detach().cpu().numpy())\n",
    "\n",
    "                    loss_val_e = np.mean( np.array( local_val_losses ) )\n",
    "                    loss_validation_num_jets[constit_num][1].append(loss_val_e)\n",
    "\n",
    "                \"\"\"\n",
    "                Run a LCT for signal vs background (supervised)\n",
    "                \"\"\"\n",
    "\n",
    "                net.eval()\n",
    "                #lct_train_reps = F.normalize( net.forward_batchwise( torch.Tensor( data_test_f ).transpose(1,2), data_test_f.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu(), dim=-1  ).numpy()\n",
    "                #lct_test_reps = F.normalize( net.forward_batchwise( torch.Tensor( data_val ).transpose(1,2), data_val.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu(), dim=-1  ).numpy()\n",
    "                lct_train_reps = net.forward_batchwise( torch.Tensor( data_test_f ).transpose(1,2), data_test_f.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu().numpy()\n",
    "                lct_test_reps = net.forward_batchwise( torch.Tensor( data_val ).transpose(1,2), data_val.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu().numpy()\n",
    "\n",
    "            \n",
    "                print(\"Doing a short LCT...\")\n",
    "\n",
    "                lct_auc_num_jets[constit_num][0].append(epoch)\n",
    "                # Need to transform the data into the representation space first\n",
    "                with torch.no_grad():\n",
    "                    for trait in range(lct_train_reps.shape[1]): # going through the layers of the transformer\n",
    "                        # run the LCT\n",
    "                        reg = LinearRegression().fit(lct_train_reps[:,trait,:], labels_test_f)\n",
    "                        # make the prediction\n",
    "                        predictions = reg.predict(lct_test_reps[:,trait,:])\n",
    "                        auc = roc_auc_score(labels_val, predictions)\n",
    "                        lct_auc_num_jets[constit_num][1+trait].append(auc)\n",
    "            \n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        print( \"JETCLR TRAINING DONE, time taken: \" + str( np.round( t1-t0, 2 ) ), flush=True)\n",
    "\n",
    "\n",
    "        # save out results\n",
    "        print( \"saving out data/results\", flush=True)\n",
    "        np.save( expt_dir+\"clr_losses_train_\"+str(constit_num)+\".npy\", losses_clr_num_jets[constit_num] )\n",
    "        np.save( expt_dir+\"clr_numer_loss_train_\"+str(constit_num)+\".npy\", losses_clr_numer_num_jets[constit_num] )\n",
    "        np.save( expt_dir+\"clr_denom_loss_train_\"+str(constit_num)+\".npy\", losses_clr_denom_num_jets[constit_num] )\n",
    "        np.save( expt_dir+\"clr_losses_val_\"+str(constit_num)+\".npy\", loss_validation_num_jets[constit_num] )\n",
    "        np.save( expt_dir+\"lct_auc_\"+str(constit_num)+\".npy\", lct_auc_num_jets[constit_num] )\n",
    "\n",
    "        # save out final trained model\n",
    "        print( \"saving out final jetCLR model\", flush=True )\n",
    "        torch.save(net.state_dict(), expt_dir+\"final_model_\"+str(constit_num)+\".pt\")\n",
    "        print()\n",
    "\n",
    "        print(\"Avg # constits:\", np.mean(mean_consts_post_split))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91182c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the training contrastive losses\n",
    "\"\"\"\n",
    "plot_clr_losses = []\n",
    "\n",
    "plot_clr_losses.append((range(len(losses_clr_num_jets[constit_num])),\n",
    "                       losses_clr_num_jets[constit_num], \"CLR loss, \"+str(constit_num) + \" constits\"))\n",
    "plot_clr_losses.append((loss_validation_num_jets[constit_num][0],\n",
    "                       loss_validation_num_jets[constit_num][1],\"Val loss, \"+str(constit_num) + \" constits\"))\n",
    "plot_losses(plot_clr_losses, \"Contrastive losses, training\", True)  \n",
    "    \n",
    "\"\"\"\n",
    "Plot the LC + NN AUC\n",
    "\"\"\"\n",
    "\n",
    "plot_LCT_stats = []\n",
    "plot_LCT_stats.append((lct_auc_num_jets[constit_num][0], lct_auc_num_jets[constit_num][1],\n",
    "                         \"LC transformer, \"+str(constit_num) + \" constits\"))\n",
    "plot_LCT_stats.append((lct_auc_num_jets[constit_num][0], lct_auc_num_jets[constit_num][2],\n",
    "                        \"LC hidden layer, \"+str(constit_num) + \" constits\"))\n",
    "plot_LCT_stats.append((lct_auc_num_jets[constit_num][0], lct_auc_num_jets[constit_num][3],\n",
    "                        \"LC output layer, \"+str(constit_num) + \" constits\"))\n",
    "\n",
    "plot_losses(plot_LCT_stats, \"ROC Area\", False)  \n",
    "\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Plot the training contrastive losses num + denom\n",
    "\"\"\"\n",
    "\n",
    "plot_num_val_losses = []\n",
    "\n",
    "plot_num_val_losses.append((range(len(losses_clr_numer_num_jets[constit_num])),\n",
    "                       -np.array(losses_clr_numer_num_jets[constit_num]), str(constit_num) + \" constits\"))\n",
    "plot_losses(plot_num_val_losses, \"-Alignment losses (should increase)\", True)  \n",
    "\n",
    "plot_den_val_losses = []\n",
    "\n",
    "plot_den_val_losses.append((range(len(losses_clr_denom_num_jets[constit_num])),\n",
    "                       np.array(losses_clr_denom_num_jets[constit_num]),  str(constit_num) + \" constits\"))\n",
    "plot_losses(plot_den_val_losses, \"Uniformity losses (should decrease)\", True)  \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f5e95-c2d7-42dc-8e34-9b5cf14dd5cf",
   "metadata": {},
   "source": [
    "# Run final LCT on the transformer representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cc28ffb-1178-41ef-b7b0-cf22f760f241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Linear(in_features=3, out_features=512, bias=True)\n",
       "  (decoder): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head_layers): ModuleList(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constit_num = grading\n",
    "\n",
    "# Loading in the final transformer\n",
    "\n",
    "loaded_net = Transformer( input_dim, model_dim, output_dim, n_heads, dim_feedforward, \n",
    "                  n_layers, learning_rate_trans, n_head_layers, dropout=0.1, opt=opt )\n",
    "\n",
    "loaded_net.load_state_dict(torch.load(expt_dir+\"final_model_\"+str(constit_num)+\".pt\"))\n",
    "loaded_net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2205eac3-c69d-4b54-9acf-abf238eb51b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into net...\n"
     ]
    }
   ],
   "source": [
    "# Running the final transformer on the binary classification data\n",
    "\n",
    "print(\"Loading data into net...\")\n",
    "#lct_train_reps = F.normalize( loaded_net.forward_batchwise( torch.Tensor( data_train ).transpose(1,2), data_train.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu(), dim=-1  ).numpy()\n",
    "#lct_test_reps = F.normalize( loaded_net.forward_batchwise( torch.Tensor( data_test_f ).transpose(1,2), data_test_f.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu(), dim=-1  ).numpy()\n",
    "\n",
    "lct_train_reps = loaded_net.forward_batchwise( torch.Tensor( data_train ).transpose(1,2), data_train.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu().numpy()\n",
    "lct_val_reps =  loaded_net.forward_batchwise( torch.Tensor( data_val ).transpose(1,2), data_val.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu().numpy()\n",
    "lct_test_reps =  loaded_net.forward_batchwise( torch.Tensor( data_test_f ).transpose(1,2), data_test_f.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu().numpy()\n",
    "\n",
    "print(\"Data loaded!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1ad8f-d110-4896-b89e-e69e603adaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "print(\"Doing a LCT...\")\n",
    "# Need to transform the data into the representation space first\n",
    "with torch.no_grad():\n",
    "    for trait in range(lct_train_reps.shape[1]): # going through the layers of the transformer\n",
    "        # run the LCT\n",
    "\n",
    "       \n",
    "        reg = LinearRegression().fit(lct_train_reps[:,trait,:], labels_train)\n",
    "        # make the prediction\n",
    "        predictions = reg.predict(lct_test_reps[:,trait,:])\n",
    "        fpr, tpr, _ = roc_curve(labels_test_f, predictions)\n",
    "        \n",
    "        plt.plot(tpr, 1.0/fpr, label = \"LCT\"+str(trait))\n",
    "        \n",
    "        np.save( expt_dir+\"CLR_LCT\"+str(trait)+\"_fpr_\"+str(constit_num)+\".npy\", fpr )\n",
    "        np.save( expt_dir+\"CLR_LCT\"+str(trait)+\"_tpr_\"+str(constit_num)+\".npy\", tpr )\n",
    "        \n",
    "        predicted = np.round(predictions).reshape(labels_test_f.size)\n",
    "        total = labels_test_f.size\n",
    "        correct = (predicted == labels_test_f).sum()    \n",
    "        \n",
    "        print(\"CLR_LCT\"+str(trait),\"acc:\",correct/total)\n",
    "        \n",
    "        \n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"1/(False Positive Rate)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "        \n",
    "print(\"LCT data saved\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5649d22a",
   "metadata": {},
   "source": [
    "# Run final NN on the transformer representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffee9e5-937c-475d-ab53-ba53f2f6dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.neural_net import create_and_run_nn\n",
    "\n",
    "\n",
    "num_epochs_nn = 400\n",
    "batch_size_nn = 400\n",
    "update_epochs_nn = 20\n",
    "input_shape = model_dim\n",
    "lr_nn = 0.00008\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "print(\"Doing a NN...\")\n",
    "# Need to transform the data into the representation space first\n",
    "#with torch.no_grad():\n",
    "for trait in range(lct_train_reps.shape[1]): # going through the layers of the transformer\n",
    "    # run the NN\n",
    "\n",
    "    performance_stats_nn = create_and_run_nn(device, input_shape, num_epochs_nn, batch_size_nn, update_epochs_nn,lr_nn, \n",
    "                                     lct_train_reps[:,trait,:], labels_train, \n",
    "                  lct_val_reps[:,trait,:], labels_val,\n",
    "                  lct_test_reps[:,trait,:], labels_test_f, True)\n",
    "\n",
    "    plt.plot(performance_stats_nn[\"tpr\"], 1.0/performance_stats_nn[\"fpr\"], label = \"NN\"+str(trait))\n",
    "\n",
    "    np.save( expt_dir+\"CLR_NN\"+str(trait)+\"_fpr_\"+str(constit_num)+\".npy\", performance_stats_nn[\"fpr\"] )\n",
    "    np.save( expt_dir+\"CLR_NN\"+str(trait)+\"_tpr_\"+str(constit_num)+\".npy\", performance_stats_nn[\"tpr\"] )\n",
    "\n",
    "    print(\"Accuracy of the network: %d %%\" % (100.00 *performance_stats_nn[\"acc\"]))\n",
    "    print(\"ROC AUC:\", performance_stats_nn[\"auc\"])\n",
    "    \n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"1/(False Positive Rate)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"NN data saved\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f863ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
