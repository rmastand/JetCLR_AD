{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e80fafd-9a03-435d-a5eb-d6132a9012e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# load standard python modules\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "\n",
    "# load torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# load custom modules required for jetCLR training\n",
    "from modules.transformer import Transformer\n",
    "from modules.losses import contrastive_loss, align_loss, uniform_loss, contrastive_loss_num_den\n",
    "from modules.perf_eval import get_perf_stats, linear_classifier_test, plot_losses\n",
    "from modules.jet_augs import remove_jet_and_rescale_pT\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c7ac9c-3b98-4534-bb70-66b0aa2d4f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "ERROR: experiment already exists, don't want to overwrite it by mistake\n",
      "experiment: dijet_dim_scan_50_0_large/0008d/\n"
     ]
    }
   ],
   "source": [
    "# More parameters / computing setup\n",
    "\n",
    "# set the number of threads that pytorch will use\n",
    "torch.set_num_threads(2)\n",
    "\n",
    "exp_id = \"dijet_dim_scan_50_0_large/0008d/\"\n",
    "\n",
    "# set gpu device\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print( \"device: \" + str( device ), flush=True)\n",
    "\n",
    "# set up results directory\n",
    "base_dir = \"/global/home/users/rrmastandrea/MJetCLR/\"  # change this to your working directory\n",
    "expt_dir = base_dir + \"projects/rep_learning/experiments/\" + exp_id + \"/\"\n",
    "\n",
    "#check if experiment alreadyexists\n",
    "if os.path.isdir(expt_dir):\n",
    "    print(\"ERROR: experiment already exists, don't want to overwrite it by mistake\")\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(expt_dir)\n",
    "\n",
    "print(\"experiment: \"+str(exp_id) , flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93478edd-8830-46e1-ab73-83083fe8cd9f",
   "metadata": {},
   "source": [
    "# Load in the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f073bead-8e91-4bcc-afb2-34cb13156afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/home/users/rrmastandrea/training_data/n_sig_17764_n_bkg_50000_n_nonzero_50_n_pad_0_n_jet_2/\n",
      "BC training data shape: (40658, 3, 100)\n",
      "BC training labels shape: (40658,)\n",
      "BC val data shape: (16263, 3, 100)\n",
      "BC val labels shape: (16263,)\n",
      "BC test data shape: (10843, 3, 100)\n",
      "BC test labels shape: (10843,)\n"
     ]
    }
   ],
   "source": [
    "path_to_save_dir = \"/global/home/users/rrmastandrea/training_data/\"\n",
    "\n",
    "\n",
    "save_id_dir = \"n_sig_17764_n_bkg_50000_n_nonzero_50_n_pad_0_n_jet_2/\"\n",
    "grading = 50\n",
    "n_jets = 2\n",
    "\n",
    "path_to_data = path_to_save_dir+save_id_dir\n",
    "print(path_to_data)\n",
    "\n",
    "\n",
    "\n",
    "data_train = np.load(path_to_data+\"data_train.npy\")\n",
    "labels_train = np.load(path_to_data+\"labels_train.npy\")\n",
    "data_val = np.load(path_to_data+\"data_val.npy\")\n",
    "labels_val = np.load(path_to_data+\"labels_val.npy\")\n",
    "data_test_f = np.load(path_to_data+\"data_test_f.npy\")\n",
    "labels_test_f = np.load(path_to_data+\"labels_test_f.npy\")\n",
    "\n",
    "\n",
    "# rescale\n",
    "data_train = remove_jet_and_rescale_pT(data_train, n_jets)\n",
    "data_val = remove_jet_and_rescale_pT(data_val, n_jets)\n",
    "data_test_f = remove_jet_and_rescale_pT(data_test_f, n_jets)\n",
    "\n",
    "\n",
    "# print data dimensions\n",
    "print( \"BC training data shape: \" + str( data_train.shape ), flush=True)\n",
    "print( \"BC training labels shape: \" + str( labels_train.shape ), flush=True)\n",
    "print( \"BC val data shape: \" + str( data_val.shape ), flush=True)\n",
    "print( \"BC val labels shape: \" + str( labels_val.shape ), flush=True)\n",
    "print( \"BC test data shape: \" + str( data_test_f.shape ), flush=True)\n",
    "print( \"BC test labels shape: \" + str( labels_test_f.shape ), flush=True)\n",
    "\n",
    "\n",
    "# Plot num constituents\n",
    "\n",
    "def get_num_constits(dataset):\n",
    "    consits_list = []\n",
    "    for collision in dataset:\n",
    "        pts = collision[0,:]\n",
    "\n",
    "        pads = np.where(pts==0)\n",
    "        consits_list.append(dataset.shape[2]-len(pads[0]))\n",
    "        \n",
    "    return consits_list\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a97a2",
   "metadata": {},
   "source": [
    "# Run the transformer architecture as a BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a92438",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the Binary Classifier transformer net\n",
    "\"\"\"\n",
    "\n",
    "# transformer hyperparams\n",
    "input_dim = 3\n",
    "model_dim = 8\n",
    "output_dim = model_dim\n",
    "dim_feedforward = model_dim\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "n_head_layers = 2\n",
    "opt = \"adam\"\n",
    "\n",
    "mask= False\n",
    "cmask = True\n",
    "\n",
    "learning_rate_trans = 0.0001\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "netBC = Transformer( input_dim, model_dim, output_dim, n_heads, dim_feedforward, \n",
    "                  n_layers, learning_rate_trans, n_head_layers, dropout=0.1, opt=opt, BC = True )\n",
    "\n",
    "## send network to device\n",
    "netBC.to( device )\n",
    "\n",
    "# define lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( netBC.optimizer, factor=0.2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be839665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training loop, running for 100 epochs with 50 constituents\n",
      "BC training data shape: (40658, 3, 100)\n",
      "BC val data shape: (16263, 3, 100)\n",
      "---\n",
      "epoch: 0, loss: 0.94993\n",
      "time taken up to now: 1.6645514965057373\n",
      "\n",
      "Getting the validation loss...\n",
      "epoch: 5, loss: 0.59539\n",
      "time taken up to now: 9.016929388046265\n",
      "\n",
      "Getting the validation loss...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-89ddc0d1f6a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mz_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mnetBC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0mnetBC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/computingML2/lib64/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/computingML2/lib64/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/computingML2/lib64/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/computingML2/lib64/python3.6/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_BC_transformer = True\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "n_constits_max = grading\n",
    "\n",
    "\n",
    "if run_BC_transformer:\n",
    "\n",
    "    # THE TRAINING LOOP\n",
    "\n",
    "    # initialise lists for storing training stats, validation loss\n",
    "    losses_BC_num_jets = {i:[] for i in range(grading,n_constits_max+grading,grading)}\n",
    "    loss_validation_num_jets = {i:[[],[]] for i in range(grading,n_constits_max+grading,grading)} #epoch, loss\n",
    "\n",
    "    n_epochs = 10\n",
    "    loss_check_epoch = 5\n",
    "    verbal_epoch = 5\n",
    "\n",
    "    for constit_num in range(grading,n_constits_max+grading,grading):\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        print( \"starting training loop, running for \" + str( n_epochs ) + \" epochs\" + \" with \" + str(constit_num) + \" constituents\" \n",
    "              , flush=True)\n",
    "        print(\"BC training data shape:\",data_train.shape)\n",
    "        print(\"BC val data shape:\",data_val.shape)\n",
    "        print( \"---\", flush=True )\n",
    "\n",
    "        # re-batch the data on each epoch\n",
    "        for epoch in range( n_epochs + 1 ):\n",
    "\n",
    "            # get batch_size number of indices\n",
    "            indices_list = torch.split( torch.randperm( data_train.shape[0] ), batch_size )\n",
    "\n",
    "            # initialise lists to store batch stats\n",
    "            losses_BC_e = []\n",
    "\n",
    "            # the inner loop goes through the dataset batch by batch\n",
    "            # augmentations of the jets are done on the fly\n",
    "            for i, indices in enumerate( indices_list ): # random jets from the dataset\n",
    "                netBC.optimizer.zero_grad()\n",
    "                \"\"\"\n",
    "                DATA PREPARATION\n",
    "                \"\"\"\n",
    "                x_i = data_train[indices,:,:]\n",
    "                labels_i = labels_train[indices]\n",
    "\n",
    "                x_i = torch.Tensor( x_i ).transpose(1,2).to( device ) # shape (batchsize, 2, 3)\n",
    "                labels_i = torch.Tensor( labels_i ).to( device )\n",
    "                z_i = netBC( x_i, use_mask=mask, use_continuous_mask=cmask ) # shape (batchsize, output_dim)\n",
    "\n",
    "                \"\"\"\n",
    "                LOSS CALCULATIONS\n",
    "                \"\"\"           \n",
    "\n",
    "                # compute the loss based on predictions of the netBC and the correct answers\n",
    "                loss = criterion( z_i, labels_i.reshape(-1,1)).to( device )\n",
    "                loss.backward()\n",
    "                netBC.optimizer.step()\n",
    "                netBC.optimizer.zero_grad()\n",
    "                \n",
    "                losses_BC_e.append( loss.detach().cpu().numpy() )\n",
    "                \n",
    "            \"\"\"\n",
    "            AVERAGING OF LOSSES\n",
    "            \"\"\" \n",
    "            loss_BC_e = np.mean( np.array( losses_BC_e ) )\n",
    "            ## scheduler\n",
    "            scheduler.step( loss_BC_e )\n",
    "\n",
    "            # storage\n",
    "            losses_BC_num_jets[constit_num].append( loss_BC_e )\n",
    "           \n",
    "            \"\"\"\n",
    "            EVERY SO OFTEN, GIVEN AN UPDATE\n",
    "            \"\"\"\n",
    "\n",
    "            if epoch % verbal_epoch == 0:\n",
    "\n",
    "                print( \"epoch: \" + str( epoch ) + \", loss: \" + str( round(losses_BC_num_jets[constit_num][-1], 5) ), flush=True )\n",
    "                print(\"time taken up to now: \" + str(time.time()-t0))\n",
    "                print()\n",
    "\n",
    "            if epoch % loss_check_epoch == 0:\n",
    "\n",
    "                \"\"\"\n",
    "                Get the validation loss\n",
    "                \"\"\"\n",
    "                print(\"Getting the validation loss...\")\n",
    "                # store the epoch\n",
    "                loss_validation_num_jets[constit_num][0].append(epoch)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    netBC.eval()\n",
    "\n",
    "                    # get batch_size number of indices\n",
    "                    indices_list_val = torch.split( torch.randperm( data_val.shape[0] ), batch_size )\n",
    "                    local_val_losses = []\n",
    "\n",
    "                    for j, indices_val in enumerate( indices_list_val ):\n",
    "                        \n",
    "                        \"\"\"\n",
    "                        DATA PREPARATION\n",
    "                        \"\"\"\n",
    "                        a_i = data_val[indices_val,:,:]\n",
    "                        labelsa_i = labels_val[indices_val]\n",
    "\n",
    "                        a_i = torch.Tensor( a_i ).transpose(1,2).to( device ) # shape (batchsize, 2, 3)\n",
    "                        labelsa_i = torch.Tensor( labelsa_i ).to( device )\n",
    "                        w_i = netBC( a_i, use_mask=mask, use_continuous_mask=cmask ) # shape (batchsize, output_dim)\n",
    "\n",
    "                        \"\"\"\n",
    "                        LOSS CALCULATIONS\n",
    "                        \"\"\"           \n",
    "\n",
    "                        # compute the loss based on predictions of the netBC and the correct answers\n",
    "                        loss_val = criterion( w_i, labelsa_i.reshape(-1,1)).to( device )\n",
    "                        local_val_losses.append(loss_val.detach().cpu().numpy())\n",
    "                \n",
    "                    loss_val_e = np.mean( np.array( local_val_losses ) )\n",
    "                    loss_validation_num_jets[constit_num][1].append(loss_val_e)\n",
    "     \n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        print( \"BC TRAINING DONE, time taken: \" + str( np.round( t1-t0, 2 ) ), flush=True)\n",
    "\n",
    "\n",
    "        # save out results\n",
    "        print( \"saving out data/results\", flush=True)\n",
    "        np.save( expt_dir+\"BC_losses_train_\"+str(constit_num)+\".npy\", losses_BC_num_jets[constit_num] )\n",
    "        np.save( expt_dir+\"BC_losses_val_\"+str(constit_num)+\".npy\", loss_validation_num_jets[constit_num] )\n",
    "\n",
    "        # save out final trained model\n",
    "        print( \"saving out final BC model\", flush=True )\n",
    "        torch.save(netBC.state_dict(), expt_dir+\"final_model_BC_\"+str(constit_num)+\".pt\")\n",
    "        print()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_pdf_name = expt_dir + \"BCLR_training_losses.pdf\"\n",
    "pp = PdfPages(losses_pdf_name)\n",
    "\n",
    "\n",
    "plot_list = [(range(n_epochs+1), losses_BC_num_jets[constit_num], \"Loss\"),\n",
    "             (loss_validation_num_jets[constit_num][0], loss_validation_num_jets[constit_num][1], \"Validation loss\")]\n",
    "fig = plot_losses(plot_list, \"\")  \n",
    "\n",
    "pp.savefig(fig)\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f3ed2",
   "metadata": {},
   "source": [
    "# Evaluate the final transformer classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a1d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the final transformer\n",
    "\n",
    "constit_num = grading\n",
    "loaded_net_BC = Transformer( input_dim, model_dim, output_dim, n_heads, dim_feedforward, \n",
    "                  n_layers, learning_rate_trans, n_head_layers, dropout=0.1, opt=opt, BC = True )\n",
    "\n",
    "loaded_net_BC.load_state_dict(torch.load(expt_dir+\"final_model_BC_\"+str(constit_num)+\".pt\"))\n",
    "loaded_net_BC.eval()\n",
    "\n",
    "loaded_net_BC.to( device )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4dbea3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Running the final transformer on the binary classification data\n",
    "\n",
    "BC_fpt_tpr = {\"fpr\":[],\"tpr\":[]}\n",
    "\n",
    "\n",
    "LCT_pdf_name = expt_dir + \"BCTrans_plots.pdf\"\n",
    "pp = PdfPages(LCT_pdf_name)\n",
    "\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "with torch.no_grad():    \n",
    "    \n",
    "    inputs = torch.Tensor( data_test_f ).transpose(1,2).to( device )\n",
    "    \n",
    "\n",
    "    outputs = loaded_net_BC( inputs, use_mask=mask, use_continuous_mask=cmask ).detach().cpu().numpy()\n",
    "    predicted = np.round(outputs).reshape(labels_test_f.size)\n",
    "    \n",
    "    # calculate auc \n",
    "    auc = roc_auc_score(labels_test_f, outputs)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(labels_test_f, outputs)\n",
    "\n",
    "    total = labels_test_f.size\n",
    "    correct = (predicted == labels_test_f).sum()    \n",
    "\n",
    "    np.save( expt_dir+\"trans_BC_fpr_\"+str(constit_num)+\".npy\", fpr )\n",
    "    np.save( expt_dir+\"trans_BC_tpr_\"+str(constit_num)+\".npy\", tpr )\n",
    "        \n",
    "        \n",
    "print(\"BC data saved\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tpr, 1.0/fpr)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"1/(False Positive Rate)\")\n",
    "plt.title(\"Transformer BC\")\n",
    "plt.show()\n",
    "pp.savefig(fig)\n",
    "pp.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1ad8f-d110-4896-b89e-e69e603adaf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca05eca-25ef-4c41-94bb-404aaae36882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffee9e5-937c-475d-ab53-ba53f2f6dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b948293-9a47-477f-a50a-9cb1375a1a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d393ec-4032-4883-9c2b-f59549a851d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f88324d-16ea-4802-8f72-acd1637ee254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5b18c-c754-4d6f-96e2-e6ceb3ea72c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d64742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3791e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775ce3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb40f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
