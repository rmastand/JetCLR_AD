{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abc9920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# load custom modules required for jetCLR training\n",
    "from modules.transformer import Transformer\n",
    "from modules.neural_net import create_and_run_nn\n",
    "\n",
    "\n",
    "\n",
    "# set gpu device\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print( \"device: \" + str( device ), flush=True)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "from numba import cuda \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cfebd2",
   "metadata": {},
   "source": [
    "# Load in the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9773a58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/home/users/rrmastandrea/training_data/CWoLa_n_sig_25000_n_bkg_0_n_nonzero_50_n_pad_0_n_jet_2/\n",
      "/global/home/users/rrmastandrea/training_data/CWoLa_n_sig_0_n_bkg_25000_n_nonzero_50_n_pad_0_n_jet_2/\n",
      "Sig data shape: (25000, 3, 102)\n",
      "Sig labels shape: (25000,)\n",
      "Bkg data shape: (25000, 3, 102)\n",
      "Sig data shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "path_to_save_dir = \"/global/home/users/rrmastandrea/training_data/\"\n",
    "\n",
    "sig_samp_id = \"CWoLa_n_sig_25000_n_bkg_0_n_nonzero_50_n_pad_0_n_jet_2/\"\n",
    "bkg_samp_id = \"CWoLa_n_sig_0_n_bkg_25000_n_nonzero_50_n_pad_0_n_jet_2/\"\n",
    "\n",
    "#TEST_dir = \"STANDARD_TEST_SET_n_sig_10k_n_bkg_10k_n_nonzero_50_n_pad_0_n_jet_2/\"\n",
    "\n",
    "\n",
    "n_constits_max = 50\n",
    "n_jets = 2\n",
    "\n",
    "path_to_sig_data = path_to_save_dir+sig_samp_id\n",
    "print(path_to_sig_data)\n",
    "path_to_bkg_data = path_to_save_dir+bkg_samp_id\n",
    "print(path_to_bkg_data)\n",
    "\n",
    "sig_data = np.load(path_to_sig_data+\"data_train.npy\")\n",
    "sig_labels = np.load(path_to_bkg_data+\"labels_train.npy\")\n",
    "bkg_data = np.load(path_to_bkg_data+\"data_train.npy\")\n",
    "bkg_labels = np.load(path_to_bkg_data+\"labels_train.npy\")\n",
    "\n",
    "# print data dimensions\n",
    "print( \"Sig data shape: \" + str( sig_data.shape ), flush=True)\n",
    "print( \"Sig labels shape: \" + str( sig_labels.shape ), flush=True)\n",
    "print( \"Bkg data shape: \" + str( bkg_data.shape ), flush=True)\n",
    "print( \"Sig data shape: \" + str( bkg_labels.shape ), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21665eed",
   "metadata": {},
   "source": [
    "# Run the data through the transformer net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be1b1513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (decoder): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head_layers): ModuleList(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the transformer net\n",
    "\n",
    "model_dim = 128\n",
    "\n",
    "# set gpu device\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print( \"device: \" + str( device ), flush=True)\n",
    "\n",
    "exp_id = \"SB_ratios_22_18_01/16kS_16kB_\"+str(model_dim)+\"d/\"\n",
    "base_dir = \"/global/home/users/rrmastandrea/MJetCLR/\"  # change this to your working directory\n",
    "expt_dir = base_dir + \"projects/rep_learning/experiments/\" + exp_id + \"/\"\n",
    "\n",
    "\n",
    "constit_num = 50\n",
    "\n",
    "input_dim = 3\n",
    "output_dim = model_dim\n",
    "dim_feedforward = model_dim\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "n_head_layers = 2\n",
    "opt = \"adam\"\n",
    "learning_rate_trans = 0.0001\n",
    "\n",
    "\n",
    "\n",
    "loaded_net_BC = Transformer( input_dim, model_dim, output_dim, n_heads, dim_feedforward, \n",
    "                  n_layers, learning_rate_trans, n_head_layers, dropout=0.1, opt=opt, BC = True )\n",
    "\n",
    "loaded_net_BC.load_state_dict(torch.load(expt_dir+\"final_model_BC_\"+str(constit_num)+\".pt\"))\n",
    "loaded_net_BC.eval()\n",
    "\n",
    "loaded_net_BC.to( device )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b30cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the final transformer on the binary classification data\n",
    "\n",
    "print(\"Loading data into net...\")\n",
    "lct_train_reps = F.normalize( loaded_net.forward_batchwise( torch.Tensor( data_train ).transpose(1,2), data_train.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu(), dim=-1  ).numpy()\n",
    "lct_val_reps = F.normalize( loaded_net.forward_batchwise( torch.Tensor( data_val ).transpose(1,2), data_test_f.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu(), dim=-1  ).numpy()\n",
    "lct_test_reps = F.normalize( loaded_net.forward_batchwise( torch.Tensor( data_test_f ).transpose(1,2), data_test_f.shape[0], use_mask=mask, use_continuous_mask=cmask ).detach().cpu(), dim=-1  ).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mixed latent-space samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9e94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CWoLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bc0fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37938685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473985f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2c37ddc",
   "metadata": {},
   "source": [
    "## Generate the signal and background samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36546da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_mean = 5\n",
    "sig_std = 5\n",
    "num_sig = 10000\n",
    "\n",
    "sig_sample = np.random.normal(sig_mean, sig_std, num_sig)\n",
    "\n",
    "bkg_mean = 10\n",
    "bkg_std = 5\n",
    "num_bkg = 10000\n",
    "\n",
    "bkg_sample = np.random.normal(bkg_mean, bkg_std, num_bkg)\n",
    "\n",
    "data_sample = np.concatenate((sig_sample, bkg_sample))\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(data_sample, bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ccce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mixed_sample(sig_set, bkg_set, N, f):\n",
    "    \n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    N: number of total events\n",
    "    f: signal fraction\n",
    "    \n",
    "    OUTPUTS\n",
    "    mixed_sample : unshuffled array of [signal, background] samples\n",
    "    \"\"\"\n",
    "    \n",
    "    num_sig = int(f*N)\n",
    "    num_bkg = N - num_sig\n",
    "    \n",
    "    sig_selection = np.random.choice(sig_set, num_sig)\n",
    "    bkg_selection = np.random.choice(bkg_set, num_bkg)\n",
    "    \n",
    "    mixed_sample = np.concatenate((sig_selection, bkg_selection))\n",
    "    \n",
    "    return mixed_sample\n",
    "        \n",
    "def generate_train_test_val(M1, M2, test_size = .9, val_size = 0.1):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    M1: np.array of data to be given the label 1\n",
    "    M2: np.array of data to be given the label 0\n",
    "    \n",
    "    OUPUTS\n",
    "    training, validation, and testing datasets + labels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # split data into train-test\n",
    "    ((sig_train, sig_test),\n",
    "     (bkg_train, bkg_test),) = [train_test_split(arr, test_size=test_size) for arr in [\n",
    "                                                M1,\n",
    "                                                M2,]]\n",
    "\n",
    "    # split train data into train-val\n",
    "    ((sig_train, sig_val),\n",
    "     (bkg_train, bkg_val),) = [train_test_split(arr, test_size=val_size) for arr in [\n",
    "                                                sig_train,\n",
    "                                                bkg_train,]]\n",
    "\n",
    "    # prepare the datasets + labels\n",
    "    data_train = np.concatenate([sig_train, bkg_train])\n",
    "    labels_train = np.concatenate([np.ones(sig_train.shape[0]),np.zeros(bkg_train.shape[0])])\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_train = np.reshape(data_train, (-1,1))\n",
    "    labels_train = np.reshape(labels_train, (-1,1))\n",
    "\n",
    "    data_val = np.concatenate([sig_val, bkg_val])\n",
    "    labels_val = np.concatenate([np.ones(sig_val.shape[0]),np.zeros(bkg_val.shape[0])])\n",
    "    data_val, labels_val = shuffle(data_val, labels_val)\n",
    "    data_val = np.reshape(data_val, (-1,1))\n",
    "    labels_val = np.reshape(labels_val, (-1,1))\n",
    "\n",
    "    data_test = np.concatenate([sig_test, bkg_test])\n",
    "    labels_test = np.concatenate([np.ones(sig_test.shape[0]),np.zeros(bkg_test.shape[0])])\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    data_test = np.reshape(data_test, (-1,1))\n",
    "    labels_test = np.reshape(labels_test, (-1,1))\n",
    "    \n",
    "    return data_train, labels_train, data_val, labels_val, data_test, labels_test\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429422a6",
   "metadata": {},
   "source": [
    "## Fully supervised classifier and CWoLa classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3054c8b6",
   "metadata": {},
   "source": [
    "### Data preparation: Fully supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b1611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "\n",
    "\n",
    "N_train = 100.0 # number of training samples\n",
    "N_test = 100000.0 # number of testing samples\n",
    "\n",
    "test_frac = N_test/(N_test+N_train)\n",
    "N_tot = N_train+N_test\n",
    "\n",
    "num_sig = int(N_tot/2)\n",
    "num_bkg = int(N_tot/2)\n",
    "\n",
    "\n",
    "ful_sup_sig = np.random.normal(sig_mean, sig_std, num_sig)\n",
    "ful_sup_bkg = np.random.normal(bkg_mean, bkg_std, num_bkg)\n",
    "\n",
    "bins = np.linspace(-10, 30, 40)\n",
    "plt.figure()\n",
    "plt.hist(ful_sup_sig, bins, label = \"Sig\", alpha = 0.4)\n",
    "plt.hist(ful_sup_bkg, bins, label = \"Bkg\", alpha = 0.4)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make the training datasets\n",
    "\n",
    "ful_sup_data_train, ful_sup_labels_train, ful_sup_data_val, ful_sup_labels_val, ful_sup_data_test, ful_sup_labels_test = generate_train_test_val(ful_sup_sig, ful_sup_bkg,\n",
    "                                                                                                                                                test_frac, .1)  \n",
    "\n",
    "\n",
    "print(\"Fully supervised training datasets:\")\n",
    "print(\"Training data shape:\", ful_sup_data_train.shape, \"Labels shape:\", ful_sup_labels_train.shape)\n",
    "print(\"Validation data shape:\", ful_sup_data_val.shape, \"Labels shape:\", ful_sup_labels_val.shape)\n",
    "print(\"Testing data shape:\", ful_sup_data_test.shape, \"Labels shape:\", ful_sup_labels_test.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(ful_sup_data_train, bins = 50)\n",
    "plt.xlabel(\"Training dataset\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec8c50",
   "metadata": {},
   "source": [
    "### Data preparation: CWoLa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "\n",
    "num_M1 = num_sig\n",
    "num_M2 = num_bkg\n",
    "\n",
    "\n",
    "sig_set = np.random.normal(sig_mean, sig_std, num_sig)\n",
    "bkg_set = np.random.normal(bkg_mean, bkg_std, num_bkg)\n",
    "\n",
    "f1 = .8\n",
    "f2 = 1.0 - f1\n",
    "\n",
    "mixed_samp_1 = generate_mixed_sample(sig_set, bkg_set, num_M1, f1)\n",
    "mixed_samp_2 = generate_mixed_sample(sig_set, bkg_set, num_M2, f2)\n",
    "\n",
    "bins = np.linspace(-10, 30, 40)\n",
    "plt.figure()\n",
    "plt.hist(mixed_samp_1, bins, label = \"M1\", alpha = 0.4)\n",
    "plt.hist(mixed_samp_2, bins, label = \"M2\", alpha = 0.4)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make the training datasets\n",
    "\n",
    "CWoLa_data_train, CWoLa_labels_train, CWoLa_data_val, CWoLa_labels_val, CWoLa_data_test, CWoLa_labels_test = generate_train_test_val(mixed_samp_1, mixed_samp_2,\n",
    "                                                                                                                                    test_frac, .1)  \n",
    "\n",
    "\n",
    "print(\"CWoLa training datasets:\")\n",
    "print(\"Training data shape:\", CWoLa_data_train.shape, \"Labels shape:\", CWoLa_labels_train.shape)\n",
    "print(\"Validation data shape:\", CWoLa_data_val.shape, \"Labels shape:\", CWoLa_labels_val.shape)\n",
    "print(\"Testing data shape:\", CWoLa_data_test.shape, \"Labels shape:\", CWoLa_labels_test.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(CWoLa_data_train, bins = 50)\n",
    "plt.xlabel(\"Training dataset\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab95515",
   "metadata": {},
   "source": [
    "### Running the binary classifier: Fully supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8003551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = 1\n",
    "num_epochs = 1000\n",
    "batch_size = 256\n",
    "update_epochs = 10\n",
    "lr = 0.0001\n",
    "\n",
    "performance_stats = create_and_run_nn(device, input_shape, num_epochs, batch_size, update_epochs, lr, \n",
    "                      ful_sup_data_train, ful_sup_labels_train, \n",
    "                      ful_sup_data_val, ful_sup_labels_val,\n",
    "                      ful_sup_data_test, ful_sup_labels_test, \n",
    "                      verbose = True, early_stop = True, LRschedule = False)\n",
    "\n",
    "\n",
    "# Plot the output losses   \n",
    "plt.figure()\n",
    "plt.plot(performance_stats[\"epochs\"],performance_stats[\"losses\"], label = \"loss\")\n",
    "plt.plot(performance_stats[\"val_epochs\"],performance_stats[\"val_losses\"], label = \"val loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.title(\"Fully Supervised\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(performance_stats[\"tpr\"], 1.0/performance_stats[\"fpr\"])\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"1/(False Positive Rate)\")\n",
    "plt.title(\"Fully Supervised\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy of the network: %d %%\" % (100.00 *performance_stats[\"acc\"]))\n",
    "print(\"ROC AUC:\", performance_stats[\"auc\"])\n",
    "\n",
    "full_sup_AUC = performance_stats[\"auc\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b529b0d6",
   "metadata": {},
   "source": [
    "### Running the binary classifier: CWoLa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = 1\n",
    "num_epochs = 1000\n",
    "batch_size = 256\n",
    "update_epochs = 10\n",
    "lr = 0.0001\n",
    "\n",
    "performance_stats = create_and_run_nn(device, input_shape, num_epochs, batch_size, update_epochs, lr, \n",
    "                      CWoLa_data_train, CWoLa_labels_train, \n",
    "                      CWoLa_data_val, CWoLa_labels_val,\n",
    "                      CWoLa_data_test, CWoLa_labels_test, \n",
    "                      verbose = True, early_stop = True, LRschedule = False)\n",
    "\n",
    "\n",
    "# Plot the output losses   \n",
    "plt.figure()\n",
    "plt.plot(performance_stats[\"epochs\"],performance_stats[\"losses\"], label = \"loss\")\n",
    "plt.plot(performance_stats[\"val_epochs\"],performance_stats[\"val_losses\"], label = \"val loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.title(\"CWoLa\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(performance_stats[\"tpr\"], 1.0/performance_stats[\"fpr\"])\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"1/(False Positive Rate)\")\n",
    "plt.title(\"CWoLa\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy of the network: %d %%\" % (100.00 *performance_stats[\"acc\"]))\n",
    "print(\"ROC AUC:\", performance_stats[\"auc\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0ed37",
   "metadata": {},
   "source": [
    "### Do the CWoLa training for a range of mixed sample fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa2c137",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Always pull from the same signal / background samples (so we don't have to regenerate)\n",
    "\n",
    "\n",
    "sig_set = np.random.normal(sig_mean, sig_std, num_sig)\n",
    "bkg_set = np.random.normal(bkg_mean, bkg_std, num_bkg)\n",
    "num_M1 = num_sig\n",
    "num_M2 = num_bkg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# NN hyperparams\n",
    "input_shape = 1\n",
    "num_epochs = 100\n",
    "batch_size = 400\n",
    "update_epochs = 5\n",
    "lr = 0.0001\n",
    "\n",
    "f1_vals = []\n",
    "ROC_AUC_vals = []\n",
    "\n",
    "for f1 in np.linspace(0,1,31):\n",
    "    \n",
    "    f1_vals.append(f1)\n",
    "    f2 = 1.0 - f1\n",
    "    \n",
    "    # Generate the mixed samples\n",
    "    mixed_samp_1 = generate_mixed_sample(sig_set, bkg_set, num_M1, f1)\n",
    "    mixed_samp_2 = generate_mixed_sample(sig_set, bkg_set, num_M2, f2)\n",
    "    \n",
    "    # Plot the mixed samples\n",
    "    bins = np.linspace(-10, 30, 40)\n",
    "    plt.figure()\n",
    "    plt.hist(mixed_samp_1, bins, label = \"M1\", alpha = 0.4)\n",
    "    plt.hist(mixed_samp_2, bins, label = \"M2\", alpha = 0.4)\n",
    "    plt.title(\"f1 = \"+str(f1))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Make the training datasets\n",
    "    CWoLa_data_train, CWoLa_labels_train, CWoLa_data_val, CWoLa_labels_val, CWoLa_data_test, CWoLa_labels_test = generate_train_test_val(mixed_samp_1, mixed_samp_2,\n",
    "                                                                                                                                        test_frac, .1)  \n",
    "\n",
    "    # Run the binary classifier\n",
    "    \n",
    "    print(\"Starting CWoLa training run with f1 =\", f1)\n",
    "\n",
    "    performance_stats = create_and_run_nn(device, input_shape, num_epochs, batch_size, update_epochs, lr, \n",
    "                          CWoLa_data_train, CWoLa_labels_train, \n",
    "                          CWoLa_data_val, CWoLa_labels_val,\n",
    "                          CWoLa_data_test, CWoLa_labels_test, \n",
    "                          verbose = False, early_stop = True, LRschedule = False)\n",
    "\n",
    "\n",
    "    # Plot the output losses   \n",
    "    plt.figure()\n",
    "    plt.plot(performance_stats[\"epochs\"],performance_stats[\"losses\"], label = \"loss\")\n",
    "    plt.plot(performance_stats[\"val_epochs\"],performance_stats[\"val_losses\"], label = \"val loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Losses\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.title(\"CWoLa\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(performance_stats[\"tpr\"], 1.0/performance_stats[\"fpr\"])\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"True Positive Rate\")\n",
    "    plt.ylabel(\"1/(False Positive Rate)\")\n",
    "    plt.title(\"CWoLa\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    ROC_AUC_vals.append(performance_stats[\"auc\"])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b64b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(f1_vals,ROC_AUC_vals, label = \"CWoLa\", color = \"b\")\n",
    "plt.scatter(f1_vals, np.full(len(f1_vals), full_sup_AUC), label = \"Full. Sup.\", color = \"k\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"f1\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"N = \"+str(N_train))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80094600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338edf0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e175d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85026322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d963b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd85b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
