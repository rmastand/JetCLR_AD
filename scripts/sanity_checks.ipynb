{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8e28f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import sys\n",
    "# load standard python modules\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# load torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# load custom modules required for jetCLR training\n",
    "from modules.jet_augs import remove_jet_and_rescale_pT\n",
    "from modules.transformer import Transformer\n",
    "from modules.losses import contrastive_loss, align_loss, uniform_loss, contrastive_loss_num_den\n",
    "from modules.perf_eval import get_perf_stats, linear_classifier_test, plot_losses\n",
    "from modules.neural_net import create_and_run_nn\n",
    "from modules.jet_vars import nsub, convert_constits_coords, mj\n",
    "from modules.utils import LRScheduler, EarlyStopping\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "from numba import cuda \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "\n",
    "\n",
    "torch.set_num_threads(2)\n",
    "\n",
    "# set gpu device\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print( \"device: \" + str( device ), flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3621b218",
   "metadata": {},
   "source": [
    "# Sanity check 1: can we realize the SOTA maxSIC with a FCN trained on the 4 classic jet observables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ffd03ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/home/users/rrmastandrea/training_data_vf/nBC_sig_85000_nBC_bkg_85000_n_nonzero_50_n_pad_0_n_jet_2/\n",
      "/global/home/users/rrmastandrea/training_data_vf/STANDARD_TEST_SET_n_sig_10k_n_bkg_10k_n_nonzero_50_n_pad_0_n_jet_2/\n",
      "Data train shape: (119000, 3, 100)\n",
      "Labels train shape: (119000,)\n",
      "Data val shape: (51000, 3, 100)\n",
      "Labels val shape: (51000,)\n",
      "STS data shape: (20000, 3, 100)\n",
      "STS labels shape: (20000,)\n",
      "Data train shape, hardest jet: (119000, 3, 50)\n",
      "Data train shape, second jet: (119000, 3, 50)\n"
     ]
    }
   ],
   "source": [
    "# Load in the full dataset\n",
    "\n",
    "path_to_save_dir = \"/global/home/users/rrmastandrea/training_data_vf/\"\n",
    "samp_id = \"nBC_sig_85000_nBC_bkg_85000_n_nonzero_50_n_pad_0_n_jet_2/\"\n",
    "TEST_dir = \"STANDARD_TEST_SET_n_sig_10k_n_bkg_10k_n_nonzero_50_n_pad_0_n_jet_2/\"\n",
    "\n",
    "n_constits_max = 50\n",
    "n_jets = 2\n",
    "\n",
    "path_to_data = path_to_save_dir+samp_id\n",
    "print(path_to_data)\n",
    "path_to_STS = path_to_save_dir+TEST_dir\n",
    "print(path_to_STS)\n",
    "\n",
    "\n",
    "data_train = np.load(path_to_data+\"data_train.npy\")\n",
    "labels_train = np.load(path_to_data+\"labels_train.npy\")\n",
    "data_val = np.load(path_to_data+\"data_val.npy\")\n",
    "labels_val = np.load(path_to_data+\"labels_val.npy\")\n",
    "STS_data = np.load(path_to_STS+\"data.npy\")\n",
    "STS_labels = np.load(path_to_STS+\"labels.npy\")\n",
    "\n",
    "\n",
    "# Crop the data, rescale pt\n",
    "cropped_data_train = remove_jet_and_rescale_pT(data_train, n_jets)\n",
    "cropped_data_val = remove_jet_and_rescale_pT(data_val, n_jets)\n",
    "cropped_STS_data = remove_jet_and_rescale_pT(STS_data, n_jets)\n",
    "\n",
    "# print data dimensions\n",
    "print( \"Data train shape: \" + str( cropped_data_train.shape ), flush=True)\n",
    "print( \"Labels train shape: \" + str( labels_train.shape ), flush=True)\n",
    "print( \"Data val shape: \" + str( cropped_data_val.shape ), flush=True)\n",
    "print( \"Labels val shape: \" + str( labels_val.shape ), flush=True)\n",
    "\n",
    "print( \"STS data shape: \" + str( cropped_STS_data.shape ), flush=True)\n",
    "print( \"STS labels shape: \" + str( STS_labels.shape ), flush=True)\n",
    "\n",
    "\n",
    "# Split the events into the hardest and second jets\n",
    "\n",
    "data_train_hardest = np.split(cropped_data_train, 2, axis = 2)[0]\n",
    "data_train_second = np.split(cropped_data_train, 2, axis = 2)[1]\n",
    "data_val_hardest = np.split(cropped_data_val, 2, axis = 2)[0]\n",
    "data_val_second = np.split(cropped_data_val, 2, axis = 2)[1]\n",
    "data_STS_hardest = np.split(cropped_STS_data, 2, axis = 2)[0]\n",
    "data_STS_second = np.split(cropped_STS_data, 2, axis = 2)[1]\n",
    "\n",
    "print( \"Data train shape, hardest jet: \" + str( data_train_hardest.shape ), flush=True)\n",
    "print( \"Data train shape, second jet: \" + str( data_train_second.shape ), flush=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ada6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating jet observables...\n",
      "Calculating mj hardest jet...\n",
      "Calculating mj second jet...\n",
      "Calculating t21 hardest jet...\n",
      "Calculating t21 second jet...\n"
     ]
    }
   ],
   "source": [
    "# Calculate the jet params t21, mass\n",
    "\n",
    "def t21(jets):\n",
    "\n",
    "    \"\"\"\n",
    "    Events is an array of size (# jets, 3,2 x # constituents)where the 1-index goes through (pT, eta, phi)\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert the pT, eta, phi coords to E, px py, px\n",
    "    jets_cartesian = convert_constits_coords(jets)\n",
    "    \n",
    "    # calculate the n_jettiness\n",
    "    taus = nsub(jets_cartesian)\n",
    "    \n",
    "    taus_21 = taus[:,0]\n",
    "    taus_32 = taus[:,1]\n",
    "    \n",
    "    return taus_21\n",
    "\n",
    "\n",
    "print(\"Calculating jet observables...\")\n",
    "\n",
    "print(\"Calculating mj hardest jet...\")\n",
    "mj_hardest_train = mj(data_train_hardest)\n",
    "mj_hardest_val = mj(data_val_hardest)\n",
    "mj_hardest_test = mj(data_STS_hardest)\n",
    "print(\"Calculating mj second jet...\")\n",
    "mj_second_train = mj(data_train_second)\n",
    "mj_second_val = mj(data_val_second)\n",
    "mj_second_test = mj(data_STS_second)\n",
    "\n",
    "print(\"Calculating t21 hardest jet...\")\n",
    "t21_hardest_train = t21(data_train_hardest)\n",
    "t21_hardest_val = t21(data_val_hardest)\n",
    "t21_hardest_test = t21(data_STS_hardest)\n",
    "print(\"Calculating t21 second jet...\")\n",
    "t21_second_train = t21(data_train_second)\n",
    "t21_second_val = t21(data_val_second)\n",
    "t21_second_test = t21(data_STS_second)\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the plots \n",
    "\n",
    "t21_bins = np.linspace(0,1,20)\n",
    "mj_bins = np.linspace(0,3,20)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(t21_hardest_train, bins = t21_bins, label = \"hardest\", alpha = 0.3)\n",
    "plt.hist(t21_second_train, bins = t21_bins, label = \"second\", alpha = 0.3)\n",
    "plt.xlabel(\"t21\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(mj_hardest_train, bins = mj_bins, label = \"hardest\", alpha = 0.3)\n",
    "plt.hist(mj_second_train, bins = mj_bins, label = \"second\", alpha = 0.3)\n",
    "plt.xlabel(\"mj\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d125ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the arrays for training the FCN\n",
    "\n",
    "# train\n",
    "t21_hardest_train = np.reshape(t21_hardest_train,(t21_hardest_train.shape[0],1))\n",
    "t21_second_train = np.reshape(t21_second_train,(t21_second_train.shape[0],1))\n",
    "mj_hardest_train = np.reshape(mj_hardest_train,(mj_hardest_train.shape[0],1))\n",
    "mj_second_train = np.reshape(mj_second_train,(mj_second_train.shape[0],1))\n",
    "all_data_train = np.concatenate((t21_hardest_train,t21_second_train,mj_hardest_train,mj_second_train), axis = 1)\n",
    "\n",
    "# val\n",
    "t21_hardest_val = np.reshape(t21_hardest_val,(t21_hardest_val.shape[0],1))\n",
    "t21_second_val = np.reshape(t21_second_val,(t21_second_val.shape[0],1))\n",
    "mj_hardest_val = np.reshape(mj_hardest_val,(mj_hardest_val.shape[0],1))\n",
    "mj_second_val = np.reshape(mj_second_val,(mj_second_val.shape[0],1))\n",
    "all_data_val = np.concatenate((t21_hardest_val,t21_second_val,mj_hardest_val,mj_second_val), axis = 1)\n",
    "\n",
    "# test\n",
    "t21_hardest_test = np.reshape(t21_hardest_test,(t21_hardest_test.shape[0],1))\n",
    "t21_second_test = np.reshape(t21_second_test,(t21_second_test.shape[0],1))\n",
    "mj_hardest_test = np.reshape(mj_hardest_test,(mj_hardest_test.shape[0],1))\n",
    "mj_second_test = np.reshape(mj_second_test,(mj_second_test.shape[0],1))\n",
    "all_data_test = np.concatenate((t21_hardest_test,t21_second_test,mj_hardest_test,mj_second_test), axis = 1)\n",
    "\n",
    "print(\"Training set shape:\", all_data_train.shape)\n",
    "print(\"Validation set shape:\", all_data_val.shape)\n",
    "print(\"Testing set shape:\", all_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e218be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the dense NN\n",
    "\n",
    "input_shape = 4\n",
    "num_epochs = 1000\n",
    "batch_size = 500\n",
    "update_epochs = 10\n",
    "lr = 0.0001\n",
    "\n",
    "performance_stats = create_and_run_nn(device, input_shape, num_epochs, batch_size, update_epochs, lr, \n",
    "                              all_data_train, labels_train, \n",
    "                              all_data_val, labels_val,\n",
    "                              all_data_test, STS_labels, \n",
    "                              verbose = True, early_stop = True, LRschedule = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "print(performance_stats[\"auc\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(performance_stats[\"epochs\"],performance_stats[\"losses\"], label = \"loss\")\n",
    "plt.plot(performance_stats[\"val_epochs\"],performance_stats[\"val_losses\"], label = \"val loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(performance_stats[\"tpr\"], 1.0/performance_stats[\"fpr\"])\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"1/(False Positive Rate)\")\n",
    "plt.show()\n",
    "\n",
    "SIC = performance_stats[\"tpr\"]/np.sqrt(performance_stats[\"fpr\"])\n",
    "plt.figure()\n",
    "plt.plot(performance_stats[\"tpr\"], SIC)\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"SIC\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f31c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "170bfd63",
   "metadata": {},
   "source": [
    "# Sanity check 2: how data-hungry is the transBC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6394bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1/3 for batch .py script\n",
    "\n",
    "data_frac = 1\n",
    "expt_dir = \"sanity_checks/\"\n",
    "model_dim  = 128\n",
    "\n",
    "\n",
    "path_to_save_dir = \"/global/home/users/rrmastandrea/training_data_vf/\"\n",
    "BC_dir = \"nBC_sig_85000_nBC_bkg_85000_n_nonzero_50_n_pad_0_n_jet_2/\"\n",
    "TEST_dir = \"STANDARD_TEST_SET_n_sig_10k_n_bkg_10k_n_nonzero_50_n_pad_0_n_jet_2/\"\n",
    "\n",
    "grading = 50\n",
    "n_constits_max = 50\n",
    "n_jets = 2\n",
    "\n",
    "path_to_BC = path_to_save_dir+BC_dir\n",
    "print(path_to_BC)\n",
    "path_to_test = path_to_save_dir+TEST_dir\n",
    "print(path_to_test)\n",
    "\n",
    "\n",
    "data_train = np.load(path_to_BC+\"data_train.npy\")\n",
    "labels_train = np.load(path_to_BC+\"labels_train.npy\")\n",
    "data_val = np.load(path_to_BC+\"data_val.npy\")\n",
    "labels_val = np.load(path_to_BC+\"labels_val.npy\")\n",
    "data_test_f = np.load(path_to_test+\"data.npy\")\n",
    "labels_test_f = np.load(path_to_test+\"labels.npy\")\n",
    "\n",
    "data_train = remove_jet_and_rescale_pT(data_train, n_jets)[:int(data_frac*data_train.shape[0])]\n",
    "data_val = remove_jet_and_rescale_pT(data_val, n_jets)[:int(data_frac*data_val.shape[0])]\n",
    "data_test_f = remove_jet_and_rescale_pT(data_test_f, n_jets)[:int(data_frac*data_test_f.shape[0])]\n",
    "\n",
    "labels_train = labels_train[:int(data_train.shape[0])]\n",
    "labels_val = labels_val[:int(data_val.shape[0])]\n",
    "labels_test_f = labels_test_f[:int(data_test_f.shape[0])]\n",
    "\n",
    "# print data dimensions\n",
    "print( \"BC training data shape: \" + str( data_train.shape ), flush=True)\n",
    "print( \"BC training labels shape: \" + str( labels_train.shape ), flush=True)\n",
    "print( \"BC val data shape: \" + str( data_val.shape ), flush=True)\n",
    "print( \"BC val labels shape: \" + str( labels_val.shape ), flush=True)\n",
    "print( \"BC test data shape: \" + str( data_test_f.shape ), flush=True)\n",
    "print( \"BC test labels shape: \" + str( labels_test_f.shape ), flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5838640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2/3 for batch .py script\n",
    "\n",
    "\n",
    "# transformer hyperparams\n",
    "input_dim = 3\n",
    "output_dim = model_dim\n",
    "dim_feedforward = model_dim\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "n_head_layers = 2\n",
    "opt = \"adam\"\n",
    "\n",
    "mask= False\n",
    "cmask = True\n",
    "\n",
    "learning_rate_trans = 0.00001\n",
    "batch_size = 400\n",
    "\n",
    "netBC = Transformer( input_dim, model_dim, output_dim, n_heads, dim_feedforward, \n",
    "                  n_layers, learning_rate_trans, n_head_layers, dropout=0.1, opt=opt, BC = True )\n",
    "\n",
    "## send network to device\n",
    "netBC.to( device )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4699135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3/3 for batch .py script\n",
    "\n",
    "\n",
    "# define lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( netBC.optimizer, factor=0.2 )\n",
    "\n",
    "run_BC_transformer = True\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "early_stop = True\n",
    "if early_stop:\n",
    "    early_stopping = EarlyStopping()\n",
    "    \n",
    "if run_BC_transformer:\n",
    "\n",
    "    # THE TRAINING LOOP\n",
    "\n",
    "    # initialise lists for storing training stats, validation loss\n",
    "    losses_BC_num_jets = {i:[] for i in range(grading,n_constits_max+grading,grading)}\n",
    "    loss_validation_num_jets = {i:[[],[]] for i in range(grading,n_constits_max+grading,grading)} #epoch, loss\n",
    "\n",
    "    n_epochs = 5000\n",
    "    loss_check_epoch = 10\n",
    "    verbal_epoch = 10\n",
    "\n",
    "    for constit_num in range(grading,n_constits_max+grading,grading):\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        print( \"starting training loop, running for \" + str( n_epochs ) + \" epochs\" + \" with \" + str(constit_num) + \" constituents\" \n",
    "              , flush=True)\n",
    "        print(\"BC training data shape:\",data_train.shape)\n",
    "        print(\"BC val data shape:\",data_val.shape)\n",
    "        print( \"---\", flush=True )\n",
    "\n",
    "        # re-batch the data on each epoch\n",
    "        for epoch in range( n_epochs + 1 ):\n",
    "\n",
    "            # get batch_size number of indices\n",
    "            indices_list = torch.split( torch.randperm( data_train.shape[0] ), batch_size )\n",
    "\n",
    "            # initialise lists to store batch stats\n",
    "            losses_BC_e = []\n",
    "\n",
    "            # the inner loop goes through the dataset batch by batch\n",
    "            # augmentations of the jets are done on the fly\n",
    "            for i, indices in enumerate( indices_list ): # random jets from the dataset\n",
    "                netBC.optimizer.zero_grad()\n",
    "                \"\"\"\n",
    "                DATA PREPARATION\n",
    "                \"\"\"\n",
    "                x_i = data_train[indices,:,:]\n",
    "                labels_i = labels_train[indices]\n",
    "\n",
    "                x_i = torch.Tensor( x_i ).transpose(1,2).to( device ) # shape (batchsize, 2, 3)\n",
    "                labels_i = torch.Tensor( labels_i ).to( device )\n",
    "                z_i = netBC( x_i, use_mask=mask, use_continuous_mask=cmask ) # shape (batchsize, output_dim)\n",
    "\n",
    "                \"\"\"\n",
    "                LOSS CALCULATIONS\n",
    "                \"\"\"           \n",
    "\n",
    "                # compute the loss based on predictions of the netBC and the correct answers\n",
    "                loss = criterion( z_i, labels_i.reshape(-1,1)).to( device )\n",
    "                loss.backward()\n",
    "                netBC.optimizer.step()\n",
    "                netBC.optimizer.zero_grad()\n",
    "                \n",
    "                losses_BC_e.append( loss.detach().cpu().numpy() )\n",
    "                \n",
    "            \"\"\"\n",
    "            AVERAGING OF LOSSES\n",
    "            \"\"\" \n",
    "            loss_BC_e = np.mean( np.array( losses_BC_e ) )\n",
    "            ## scheduler\n",
    "            scheduler.step( loss_BC_e )\n",
    "\n",
    "            # storage\n",
    "            losses_BC_num_jets[constit_num].append( loss_BC_e )\n",
    "           \n",
    "            \"\"\"\n",
    "            EVERY SO OFTEN, GIVEN AN UPDATE\n",
    "            \"\"\"\n",
    "\n",
    "            if epoch % verbal_epoch == 0:\n",
    "\n",
    "                print( \"epoch: \" + str( epoch ) + \", loss: \" + str( round(losses_BC_num_jets[constit_num][-1], 5) ), flush=True )\n",
    "                print(\"time taken up to now: \" + str(time.time()-t0))\n",
    "                print()\n",
    "\n",
    "            if epoch % loss_check_epoch == 0:\n",
    "\n",
    "                \"\"\"\n",
    "                Get the validation loss\n",
    "                \"\"\"\n",
    "                print(\"Getting the validation loss...\")\n",
    "                # store the epoch\n",
    "                loss_validation_num_jets[constit_num][0].append(epoch)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    netBC.eval()\n",
    "\n",
    "                    # get batch_size number of indices\n",
    "                    indices_list_val = torch.split( torch.randperm( data_val.shape[0] ), batch_size )\n",
    "                    local_val_losses = []\n",
    "\n",
    "                    for j, indices_val in enumerate( indices_list_val ):\n",
    "                        \n",
    "                        \"\"\"\n",
    "                        DATA PREPARATION\n",
    "                        \"\"\"\n",
    "                        a_i = data_val[indices_val,:,:]\n",
    "                        labelsa_i = labels_val[indices_val]\n",
    "\n",
    "                        a_i = torch.Tensor( a_i ).transpose(1,2).to( device ) # shape (batchsize, 2, 3)\n",
    "                        labelsa_i = torch.Tensor( labelsa_i ).to( device )\n",
    "                        w_i = netBC( a_i, use_mask=mask, use_continuous_mask=cmask ) # shape (batchsize, output_dim)\n",
    "\n",
    "                        \"\"\"\n",
    "                        LOSS CALCULATIONS\n",
    "                        \"\"\"           \n",
    "\n",
    "                        # compute the loss based on predictions of the netBC and the correct answers\n",
    "                        loss_val = criterion( w_i, labelsa_i.reshape(-1,1)).to( device )\n",
    "                        local_val_losses.append(loss_val.detach().cpu().numpy())\n",
    "                \n",
    "                    loss_val_e = np.mean( np.array( local_val_losses ) )\n",
    "                    loss_validation_num_jets[constit_num][1].append(loss_val_e)\n",
    "                  \n",
    "                    if early_stop:\n",
    "                        early_stopping(loss_val_e)\n",
    "            \n",
    "            if early_stopping.early_stop:\n",
    "                break\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        print( \"BC TRAINING DONE, time taken: \" + str( np.round( t1-t0, 2 ) ), flush=True)\n",
    "\n",
    "\n",
    "        # save out results\n",
    "        print( \"saving out data/results\", flush=True)\n",
    "        np.save( expt_dir+\"BC_losses_train_\"+str(data_frac)+\".npy\", losses_BC_num_jets[constit_num] )\n",
    "        np.save( expt_dir+\"BC_losses_val_\"+str(data_frac)+\".npy\", loss_validation_num_jets[constit_num] )\n",
    "\n",
    "        # save out final trained model\n",
    "        print( \"saving out final BC model\", flush=True )\n",
    "        torch.save(netBC.state_dict(), expt_dir+\"final_model_BC_\"+str(data_frac)+\".pt\")\n",
    "        print()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d45c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the final transformer and plotting\n",
    "\n",
    "data_fracs = [0.25, .5, .75, 1]\n",
    "fpr_dict_transBC = {}\n",
    "tpr_dict_transBC = {}\n",
    "\n",
    "\n",
    "for data_frac in data_fracs:\n",
    "\n",
    "\n",
    "    loaded_net_BC = Transformer( input_dim, model_dim, output_dim, n_heads, dim_feedforward, \n",
    "                      n_layers, learning_rate_trans, n_head_layers, dropout=0.1, opt=opt, BC = True )\n",
    "\n",
    "    loaded_net_BC.load_state_dict(torch.load(expt_dir+\"final_model_BC_\"+str(data_frac)+\".pt\"))\n",
    "    loaded_net_BC.eval()\n",
    "\n",
    "    loaded_net_BC.to( device )\n",
    "\n",
    "    print(\"Evaluating...\")\n",
    "    \n",
    "    with torch.no_grad():    \n",
    "\n",
    "        inputs = torch.Tensor( data_test_f ).transpose(1,2).to( device )\n",
    "\n",
    "        outputs = loaded_net_BC( inputs, use_mask=mask, use_continuous_mask=cmask ).detach().cpu().numpy()\n",
    "        predicted = np.round(outputs).reshape(labels_test_f.size)\n",
    "\n",
    "        # calculate auc \n",
    "        auc = roc_auc_score(labels_test_f, outputs)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(labels_test_f, outputs)\n",
    "\n",
    "        total = labels_test_f.size\n",
    "        correct = (predicted == labels_test_f).sum()   \n",
    "        \n",
    "        fpr_dict_transBC[data_frac] = fpr\n",
    "        tpr_dict_transBC[data_frac] = tpr\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01038f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "for data_frac in data_fracs:\n",
    "    print(\"Fraction of total LHCO Events:\", data_frac,\"; ROC AUC:\", metrics.auc(fpr_dict[data_frac], tpr_dict[data_frac]))\n",
    "    plt.plot(tpr_dict_transBC[data_frac], 1.0/fpr_dict_transBC[data_frac], label = data_frac)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"1/(False Positive Rate)\")\n",
    "plt.legend()\n",
    "plt.title(\"TransBC architecture\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45354e79",
   "metadata": {},
   "source": [
    "# Sanity check 3: how data-hungry is the FCN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a31cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frac = 1\n",
    "\n",
    "\n",
    "path_to_save_dir = \"/global/home/users/rrmastandrea/training_data_vf/\"\n",
    "BC_dir = \"nBC_sig_85000_nBC_bkg_85000_n_nonzero_50_n_pad_0_n_jet_2/\"\n",
    "TEST_dir = \"STANDARD_TEST_SET_n_sig_10k_n_bkg_10k_n_nonzero_50_n_pad_0_n_jet_2/\"\n",
    "\n",
    "grading = 50\n",
    "n_constits_max = 50\n",
    "n_jets = 2\n",
    "\n",
    "path_to_BC = path_to_save_dir+BC_dir\n",
    "print(path_to_BC)\n",
    "\n",
    "path_to_test = path_to_save_dir+TEST_dir\n",
    "print(path_to_test)\n",
    "\n",
    "\n",
    "data_train = np.load(path_to_BC+\"data_train.npy\")\n",
    "labels_train = np.load(path_to_BC+\"labels_train.npy\")\n",
    "data_val = np.load(path_to_BC+\"data_val.npy\")\n",
    "labels_val = np.load(path_to_BC+\"labels_val.npy\")\n",
    "data_test_f = np.load(path_to_test+\"data.npy\")\n",
    "labels_test_f = np.load(path_to_test+\"labels.npy\")\n",
    "\n",
    "data_train = remove_jet_and_rescale_pT(data_train, n_jets)\n",
    "data_val = remove_jet_and_rescale_pT(data_val, n_jets)\n",
    "data_test_f = remove_jet_and_rescale_pT(data_test_f, n_jets)\n",
    "\n",
    "labels_train = labels_train\n",
    "labels_val = labels_val\n",
    "labels_test_f = labels_test_f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_train = np.reshape(data_train,(data_train.shape[0],300))\n",
    "data_val = np.reshape(data_val,(data_val.shape[0],300))\n",
    "data_test_f = np.reshape(data_test_f,(data_test_f.shape[0],300))\n",
    "\n",
    "# print data dimensions\n",
    "print( \"BC training data shape: \" + str( data_train.shape ), flush=True)\n",
    "print( \"BC training labels shape: \" + str( labels_train.shape ), flush=True)\n",
    "print( \"BC val data shape: \" + str( data_val.shape ), flush=True)\n",
    "print( \"BC val labels shape: \" + str( labels_val.shape ), flush=True)\n",
    "print( \"BC test data shape: \" + str( data_test_f.shape ), flush=True)\n",
    "print( \"BC test labels shape: \" + str( labels_test_f.shape ), flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8596c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs_nn = 1200\n",
    "batch_size_nn = 400\n",
    "update_epochs_nn = 10\n",
    "input_shape = 300\n",
    "lr_nn = 0.0001\n",
    "\n",
    "\n",
    "\n",
    "fpr_dict_FCN = {}\n",
    "tpr_dict_FCN = {}\n",
    "\n",
    "\n",
    "for data_frac in data_fracs:\n",
    "    \n",
    "    print(data_frac)\n",
    "    \n",
    "    loc_data_train = data_train[:int(data_frac*data_train.shape[0])]\n",
    "    loc_data_val = data_val[:int(data_frac*data_val.shape[0])]\n",
    "    loc_data_test_f = data_test_f[:int(data_frac*data_test_f.shape[0])]\n",
    "\n",
    "    loc_labels_train = labels_train[:int(loc_data_train.shape[0])]\n",
    "    loc_labels_val = labels_val[:int(loc_data_val.shape[0])]\n",
    "    loc_labels_test_f = labels_test_f[:int(loc_data_test_f.shape[0])]\n",
    "    \n",
    "    print( \"BC training data shape: \" + str( loc_data_train.shape ), flush=True)\n",
    "    print( \"BC training labels shape: \" + str( loc_labels_train.shape ), flush=True)\n",
    "    print( \"BC val data shape: \" + str( loc_data_val.shape ), flush=True)\n",
    "    print( \"BC val labels shape: \" + str( loc_labels_val.shape ), flush=True)\n",
    "    print( \"BC test data shape: \" + str( loc_data_test_f.shape ), flush=True)\n",
    "    print( \"BC test labels shape: \" + str( loc_labels_test_f.shape ), flush=True)\n",
    "\n",
    "    performance_stats_nn = create_and_run_nn(device, input_shape, num_epochs_nn, batch_size_nn, update_epochs_nn,lr_nn, \n",
    "                                             loc_data_train, loc_labels_train, \n",
    "                          loc_data_val, loc_labels_val,\n",
    "                          loc_data_test_f, loc_labels_test_f, True)\n",
    "    \n",
    "    fpr_dict_FCN[data_frac] = performance_stats_nn[\"fpr\"]\n",
    "    tpr_dict_FCN[data_frac] = performance_stats_nn[\"tpr\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "for data_frac in data_fracs:\n",
    "    print(\"Fraction of total LHCO Events:\", data_frac,\"; ROC AUC:\", metrics.auc(fpr_dict[data_frac], tpr_dict[data_frac]))\n",
    "    plt.plot(tpr_dict_FCN[data_frac], 1.0/fpr_dict_FCN[data_frac], label = data_frac)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"1/(False Positive Rate)\")\n",
    "plt.title(\"FCN architecture\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce0fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
