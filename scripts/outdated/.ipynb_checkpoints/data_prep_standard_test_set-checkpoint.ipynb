{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e80fafd-9a03-435d-a5eb-d6132a9012e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# load standard python modules\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# load torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from modules.file_readers import phi_wrap, pandas_to_unclustered_particles, get_highest_mass_constituents, pandas_to_features, select_jets_1, select_jets_2 \n",
    "from modules.jet_visualizers import plot_jets_phase_plane, plot_nsubs\n",
    "from modules.jet_augs import apply_single_jet_augs, translate_jets, rotate_jets, rescale_pts, distort_jets, collinear_fill_jets, crop_jets\n",
    "from modules.jet_vars import nsub, convert_constits_coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f579ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose from: \"02092021\", \"dijet\"\n",
    "\n",
    "study_type = \"dijet\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93478edd-8830-46e1-ab73-83083fe8cd9f",
   "metadata": {},
   "source": [
    "# Load in the data\n",
    "\n",
    "Starts with a dataset of particles $p_T$, $\\eta$, $\\phi$\n",
    "\n",
    "Clusters into jets, takes highest mass jet, returns the constituents ordered by pT\n",
    "\n",
    "Dataset shape: (n,3,101) = (n,[$p_T$, $\\eta$, $\\phi$],1 jet + 100 constituents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f073bead-8e91-4bcc-afb2-34cb13156afd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clustered_npy_exists = True\n",
    "\n",
    "path_to_unclustered = \"/clusterfs/ml4hep/rrmastandrea/LHC0/events_anomalydetection.h5\"\n",
    "path_to_data_storage = \"/clusterfs/ml4hep/rrmastandrea/processed_data/\"+study_type+\"/\"\n",
    "\n",
    "if not clustered_npy_exists:\n",
    "    \n",
    "    start = 300000\n",
    "    stop = 305000\n",
    "    jetR = 0.8\n",
    "    j_per_e = 2\n",
    "    center = \"J1_phi_only_pi_2\"\n",
    "    ncon_store = 50\n",
    "    \n",
    "    fname_data = \"dijet_data_jetR_\"+str(jetR)+\"_\"+center+\"_\"+str(start)+\"_\"+str(stop)+\".npy\"\n",
    "    fname_labels = \"dijet_labels_jetR_\"+str(jetR)+\"_\"+center+\"_\"+str(start)+\"_\"+str(stop)+\".npy\"\n",
    "    \n",
    "    \n",
    "    print(\"Reading in unclustered events...\")\n",
    "    # Read in the file\n",
    "    unclustered_particles_data = pd.read_hdf(path_to_unclustered,start = start, stop = stop)\n",
    "    # Convert pd to numpy; get labels\n",
    "    unclustered_collisions, unclustered_particles_labels = pandas_to_unclustered_particles(unclustered_particles_data)\n",
    "    # Cluster into jets, get highest mass constituents\n",
    "    high_mass_consits_wjet, bad_indices = get_highest_mass_constituents(unclustered_collisions, jetR, center = center, j_per_e = j_per_e, ncon_store=ncon_store)\n",
    "    high_mass_labels = np.delete(unclustered_particles_labels,bad_indices)\n",
    "    np.save(path_to_data_storage+fname_data, high_mass_consits_wjet)\n",
    "    np.save(path_to_data_storage+fname_labels, high_mass_labels)\n",
    "    print(\"Saved file \"+fname_data)\n",
    "    print(\"Saved file \"+fname_labels)\n",
    "    \n",
    "    print(high_mass_consits_wjet.shape,unclustered_particles_labels.shape)\n",
    "\n",
    "    \n",
    "if clustered_npy_exists: \n",
    "    \n",
    "    jetR = 0.8\n",
    "    \n",
    "    j_per_e = 2\n",
    "    center = \"J1_phi_only_pi_2\"\n",
    "    \n",
    "\n",
    "    if study_type == \"02092021\":\n",
    "        starts_and_stops = [(0,5000),(5000,10000),(10000,15000),(15000,20000),\n",
    "                       (20000,25000),(25000,30000),(30000,35000),(35000,40000),\n",
    "                       (40000,45000),(45000,50000),(50000,55000),(55000,60000),\n",
    "                      (60000,65000),(65000,70000),(70000,75000)]#,(75000,80000),\n",
    "                       #(80000,85000),(85000,90000),(90000,95000),(95000,100000)]\n",
    "    elif study_type == \"dijet\":\n",
    "    \n",
    "        starts_and_stops = [(0,2000),(2000,6000),(6000,10000),(10000,15000),\n",
    "                            (15000,20000), (20000,25000), (25000,30000),(30000,35000),\n",
    "                            (35000,40000),(40000,45000),(50000,55000), (55000,60000),\n",
    "                            (60000,65000),(65000,70000),(70000,75000),(75000,80000),\n",
    "                            (80000,85000),(85000,90000),(90000,95000),(95000,100000),\n",
    "                           (100000,105000),(105000,110000),(110000,115000),(115000,120000),\n",
    "                           (120000,125000),(125000,130000),(130000,135000),(135000,140000),\n",
    "                           (140000,145000),(145000,150000),(150000,155000),(155000,160000),\n",
    "                           (160000,165000),(165000,170000),(170000,175000),(175000,180000),\n",
    "                           (180000,185000),(185000,190000),(190000,195000),(195000,200000),\n",
    "                           (200000,205000),(205000,210000),(210000,215000),(215000,220000),\n",
    "                           (220000,225000),(225000,230000),(230000,235000),(235000,240000),\n",
    "                           (240000,245000),(245000,250000),(250000,255000),(255000,260000),\n",
    "                           (260000,265000),(265000,270000),(270000,275000),(275000,280000),\n",
    "                           (280000,285000),(285000,290000),(290000,295000),(295000,300000),\n",
    "                           (300000,305000)]\n",
    "\n",
    "    \"\"\"\n",
    "    high_mass_consits_wjet = np.load(path_to_data_storage+\"highmassconstits_data_jetR_\"+str(jetR)+\"_deltaJ_\"+str(starts_and_stops[0][0])+\"_\"+str(starts_and_stops[0][1])+\".npy\")\n",
    "    high_mass_labels = np.load(path_to_data_storage+\"highmassconstits_labels_jetR_\"+str(jetR)+\"_deltaJ_\"+str(starts_and_stops[0][0])+\"_\"+str(starts_and_stops[0][1])+\".npy\")\n",
    "\n",
    "    for ss in starts_and_stops[1:]:\n",
    "        high_mass_consits_wjet = np.concatenate([high_mass_consits_wjet, np.load(path_to_data_storage+\"highmassconstits_data_jetR_\"+str(jetR)+\"_deltaJ_\"+str(ss[0])+\"_\"+str(ss[1])+\".npy\")])\n",
    "        high_mass_labels = np.concatenate([high_mass_labels, np.load(path_to_data_storage+\"highmassconstits_labels_jetR_\"+str(jetR)+\"_deltaJ_\"+str(ss[0])+\"_\"+str(ss[1])+\".npy\")])\n",
    "    \"\"\"\n",
    "        \n",
    "    high_mass_consits_wjet = np.load(path_to_data_storage+\"dijet_data_jetR_\"+str(jetR)+\"_\"+center+\"_\"+str(starts_and_stops[0][0])+\"_\"+str(starts_and_stops[0][1])+\".npy\")\n",
    "    high_mass_labels = np.load(path_to_data_storage+\"dijet_labels_jetR_\"+str(jetR)+\"_\"+center+\"_\"+str(starts_and_stops[0][0])+\"_\"+str(starts_and_stops[0][1])+\".npy\")\n",
    "\n",
    "   \n",
    "    for ss in starts_and_stops[1:]:\n",
    "        high_mass_consits_wjet = np.concatenate([high_mass_consits_wjet, np.load(path_to_data_storage+\"dijet_data_jetR_\"+str(jetR)+\"_\"+center+\"_\"+str(ss[0])+\"_\"+str(ss[1])+\".npy\")])\n",
    "        high_mass_labels = np.concatenate([high_mass_labels, np.load(path_to_data_storage+\"dijet_labels_jetR_\"+str(jetR)+\"_\"+center+\"_\"+str(ss[0])+\"_\"+str(ss[1])+\".npy\")])\n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "    print(\"Read in files\")\n",
    "    print(\"Data shape: \",high_mass_consits_wjet.shape)\n",
    "    print(\"Labels shape:\", high_mass_labels.shape)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8316a-1524-4b23-a700-57e3d7991fcc",
   "metadata": {},
   "source": [
    "## Jet cuts \n",
    "\n",
    "Cut on $p_t$, $\\eta$ of the jets \n",
    "\n",
    "Then split into signal and background datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556be817-36d9-4bfc-a81d-8ca1787e6ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select jets of interest\n",
    "\n",
    "# pT, eta cuts\n",
    "pt_cut_0 = [800,3000]\n",
    "eta_cut_0 = [-3,3]\n",
    "pt_cut_1 = [800,3000]\n",
    "eta_cut_1 = [-3,3]\n",
    "\n",
    "\n",
    "if study_type == \"02092021\":\n",
    "    high_mass_consits_wjet_cut, high_mass_labels_cut = select_jets_1(high_mass_consits_wjet, high_mass_labels, pt_cut_0, eta_cut_0)\n",
    "\n",
    "\n",
    "elif study_type == \"dijet\":\n",
    "    n_const = 50\n",
    "    high_mass_consits_wjet_cut, high_mass_labels_cut = select_jets_2(high_mass_consits_wjet, high_mass_labels, n_const, pt_cut_0, pt_cut_1, eta_cut_0, eta_cut_1)\n",
    "\n",
    "\n",
    "\n",
    "# plot the jet parameters for jet 1\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(high_mass_consits_wjet[:,0,0], bins = np.linspace(0,3000,60), alpha = 0.4, label = \"PRE\")\n",
    "plt.hist(high_mass_consits_wjet_cut[:,0,0], bins = np.linspace(0,3000,60), alpha = 0.4, label = \"POST\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Hardest jet $p_T$ [GeV]\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(high_mass_consits_wjet[:,1,0], bins = np.linspace(-3,3,60), alpha = 0.4, label = \"PRE\")\n",
    "plt.hist(high_mass_consits_wjet_cut[:,1,0], bins = np.linspace(-3,3,60), alpha = 0.4, label = \"POST\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Hardest jet $eta$\")\n",
    "plt.show()\n",
    "\n",
    "if study_type == \"dijet\":\n",
    "\n",
    "    # plot the jet parameters for jet 2\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(high_mass_consits_wjet[:,0,n_const+1], bins = np.linspace(0,3000,60), alpha = 0.4, label = \"PRE\")\n",
    "    plt.hist(high_mass_consits_wjet_cut[:,0,n_const+1], bins = np.linspace(0,3000,60), alpha = 0.4, label = \"POST\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Second jet $p_T$ [GeV]\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(high_mass_consits_wjet[:,1,n_const+1], bins = np.linspace(-3,3,60), alpha = 0.4, label = \"PRE\")\n",
    "    plt.hist(high_mass_consits_wjet_cut[:,1,n_const+1], bins = np.linspace(-3,3,60), alpha = 0.4, label = \"POST\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Second jet $eta$\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e864bfd",
   "metadata": {},
   "source": [
    "# Last data preprocessing\n",
    "\n",
    "-- removing the jet from the jet + constituents array\n",
    "\n",
    "-- rescaling the pt\n",
    "\n",
    "-- splitting into signal vs background\n",
    "\n",
    "-- crop the jets\n",
    "\n",
    "-- adding zero pad for collinear splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f002c-4bb0-4261-9546-c9de0d8c9629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad_consts(data, n):\n",
    "        zero_pad = np.zeros((data.shape[0],3,n-data.shape[2]))\n",
    "        data = np.concatenate((data,zero_pad), axis = 2)\n",
    "        return data\n",
    "\n",
    "j_per_e  = 2\n",
    "\n",
    "print(\"Initial array shape:\", high_mass_consits_wjet_cut.shape)\n",
    "\n",
    "# split the event into the jets\n",
    "split_jets = np.split(high_mass_consits_wjet_cut, j_per_e, axis = 2)\n",
    "modified_jets = []\n",
    "\n",
    "# now go through each jet\n",
    "for i, subjet in enumerate(split_jets):\n",
    "    print(\"On jet\", i)\n",
    "    \n",
    "    # Take only the constituents (i.e. drop the 0th, which is the jet)\n",
    "    #subjet = subjet[:,:,1:]\n",
    "    \n",
    "    # crop the n constituents\n",
    "    n_nonzero_conts = 50\n",
    "    print(\"Cropping the jets to\",n_nonzero_conts,\"consituents\")\n",
    "    subjet = crop_jets(subjet,1+n_nonzero_conts)\n",
    "    \n",
    "    # add zero pad\n",
    "    n_zero_pad = 0\n",
    "    print(\"Adding a zero pad of size\", n_zero_pad)\n",
    "    subjet = zero_pad_consts(subjet,n_zero_pad+n_nonzero_conts+1)\n",
    "    \n",
    "    modified_jets.append(subjet)\n",
    "    print()\n",
    "    \n",
    "# recombine the jets\n",
    "high_mass_consits = np.concatenate(modified_jets, axis = 2)\n",
    "print(\"Final array shape:\", high_mass_consits.shape)\n",
    "print()\n",
    "\n",
    "jet_to_plot = high_mass_consits[2]\n",
    "print(jet_to_plot.shape) \n",
    "plot_jets_phase_plane(jet_to_plot, jet_to_plot,2,(-3,3),(-3,3))   \n",
    "\n",
    "\n",
    "# get max, min pt for normalizing\n",
    "max_pt = np.max(high_mass_consits[:,0,:])\n",
    "max_eta = np.max(high_mass_consits[:,1,:])\n",
    "max_phi = np.max(high_mass_consits[:,2,:])\n",
    "                  \n",
    "print(\"Max pt:\",max_pt,\"; max eta:\", max_eta, \"; max phi:\", max_phi)\n",
    "\n",
    "# rescale the pts here\n",
    "\n",
    "rescale_denom_pt = max_pt/10\n",
    "#print(\"Rescaling all pTs by\",rescale_denom_pt)\n",
    "#high_mass_consits = rescale_pts( high_mass_consits, rescale_denom_pt ) \n",
    "\n",
    "# split into signal vs background\n",
    "\n",
    "\n",
    "high_mass_consits_sig = high_mass_consits[np.where(high_mass_labels_cut==1)]\n",
    "high_mass_consits_bkg = high_mass_consits[np.where(high_mass_labels_cut==0)]\n",
    "\n",
    "\n",
    "print(\"Signal shape:\", high_mass_consits_sig.shape)\n",
    "print(\"Background shape:\", high_mass_consits_bkg.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5fe219-708a-4073-9a5b-94b786932c72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plots of pt, eta, phi for the constituents\n",
    "\n",
    "N_start = 0  # Number of collision events\n",
    "N_stop = 70000\n",
    "\n",
    "M = 100  # Number of constituents\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "loc_plot = np.reshape(high_mass_consits_sig[N_start:N_stop,0,0:M],((N_start-N_stop)*(M),))\n",
    "plt.hist(loc_plot[loc_plot != 0]/1.0, bins = np.linspace(0,300,30), alpha = .3, density=True, label = \"signal\")\n",
    "loc_plot = np.reshape(high_mass_consits_bkg[N_start:N_stop,0,0:M],((N_start-N_stop)*(M),))\n",
    "plt.hist(loc_plot[loc_plot != 0]/1.0, bins = np.linspace(0,300,30), alpha = .3, density=True, label = \"background\")\n",
    "plt.xlabel(\"Constituent $p_T$\")\n",
    "#plt.xlim(0,5000)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "loc_plot = np.reshape(high_mass_consits_sig[N_start:N_stop,1,0:M],((N_start-N_stop)*(M),))\n",
    "plt.hist(loc_plot[loc_plot != 0], alpha = .3, density=True, label = \"signal\")\n",
    "loc_plot = np.reshape(high_mass_consits_bkg[N_start:N_stop,1,0:M],((N_start-N_stop)*(M),))\n",
    "plt.hist(loc_plot[loc_plot != 0], alpha = .3, density=True, label = \"background\")\n",
    "plt.xlabel(\"Constituent $\\eta-\\eta_J$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "loc_plot = np.reshape(high_mass_consits_sig[N_start:N_stop,2,0:M],((N_start-N_stop)*(M),))\n",
    "plt.hist(loc_plot[loc_plot != 0], alpha = .3, density=True, label = \"signal\")\n",
    "loc_plot = np.reshape(high_mass_consits_bkg[N_start:N_stop,2,0:M],((N_start-N_stop)*(M),))\n",
    "plt.hist(loc_plot[loc_plot != 0], alpha = .3, density=True, label = \"background\")\n",
    "plt.xlabel(\"Constituent $\\phi-\\phi_J$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot num constituents\n",
    "\n",
    "def get_num_constits(dataset):\n",
    "    consits_list = []\n",
    "    for collision in dataset:\n",
    "        pts = collision[0,:]\n",
    "\n",
    "        pads = np.where(pts==0)\n",
    "        consits_list.append(dataset.shape[2]-len(pads[0]))\n",
    "        \n",
    "    return consits_list\n",
    "        \n",
    "\n",
    "plt.figure()\n",
    "plt.hist(get_num_constits(high_mass_consits_sig), alpha = .3, density=True, label = \"signal\")\n",
    "plt.hist(get_num_constits(high_mass_consits_bkg), alpha = .3, density=True, label = \"background\")\n",
    "plt.xlabel(\"Num. constituents\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed59a4e-3247-49a2-909c-d9bac396cab1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Split the data into test / train / validation sets\n",
    "\n",
    "- CLR training: First ```num_clr_train``` background events\n",
    "- CLR val: 20% of CLR train\n",
    "\n",
    "- Classification training: (up to) ```njets_sig``` signal, ```njets_bkg``` background\n",
    "- Classification test: 30% of training\n",
    "- Classification test is further split into validation / test\n",
    "\n",
    "** MUST start at 10k events in to avoid overlap with the standard test set **\n",
    "\n",
    "Note: we change the S/B ratio for the CLR training ONLY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d14a7-86b8-4f05-8e9a-546ece4a9b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training dataset\n",
    "\n",
    "sts_start = 0\n",
    "\n",
    "n_constits_max = n_nonzero_conts\n",
    "\n",
    "\"\"\"\n",
    "RUN THE JET SELECTOR CODE\n",
    "\"\"\"\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "\"\"\"\n",
    "Make the CLR train and val datasets\n",
    "\"\"\"\n",
    "num_clr_train_sig = 0\n",
    "num_clr_train_bkg = 10\n",
    "val_size = 0.3\n",
    "dataset_clr_sig = high_mass_consits_sig[sts_start:num_clr_train_sig+sts_start,:,:]\n",
    "dataset_clr_bkg = high_mass_consits_bkg[sts_start:num_clr_train_bkg+sts_start,:,:]\n",
    "\n",
    "print(\"Num signal:\", dataset_clr_sig.shape[0],\"; Num background:\", dataset_clr_bkg.shape[0])\n",
    "\n",
    "\"\"\"\n",
    "# split into train - val\n",
    "((clr_sig_train, clr_sig_val),\n",
    " (clr_bkg_train, clr_bkg_val),\n",
    " ) = [train_test_split(arr, test_size=val_size) for arr in [\n",
    "    dataset_clr_sig,\n",
    "    dataset_clr_bkg,\n",
    "]]\n",
    "\"\"\"\n",
    "\n",
    "# preparing the training dataset w/ labels\n",
    "#clr_train = np.concatenate([clr_sig_train,clr_bkg_train])\n",
    "clr_train = np.concatenate([clr_bkg_train])\n",
    "clr_train = shuffle(clr_train)\n",
    "\n",
    "# preparing the test dataset(s)\n",
    "#clr_val = np.concatenate([clr_sig_val,clr_bkg_val])\n",
    "clr_val = np.concatenate([clr_bkg_val])\n",
    "clr_val = shuffle(clr_val)\n",
    "\n",
    "\"\"\"\n",
    "Make the classification datasets\n",
    "\"\"\"\n",
    "njets_sig = 16000\n",
    "njets_bkg = 16000\n",
    "\n",
    "# take only however many constituents and jets we want \n",
    "dataset_sample_sig = high_mass_consits_sig[sts_start:njets_sig+sts_start,:,:]\n",
    "dataset_sample_bkg = high_mass_consits_bkg[sts_start:njets_bkg+sts_start,:,:]\n",
    "\n",
    "\n",
    "print(\"Num signal:\", dataset_sample_sig.shape[0],\"; Num background:\", dataset_sample_bkg.shape[0])\n",
    "\n",
    "#\"\"\"\n",
    "#\"\"\"\n",
    "# STANDARD TEST SET\n",
    "#\"\"\"\n",
    "#\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "data_test = np.concatenate([dataset_sample_sig,dataset_sample_bkg])\n",
    "labels_test = np.concatenate([np.ones(dataset_sample_sig.shape[0]),np.zeros(dataset_sample_bkg.shape[0])])\n",
    "data_test, labels_test = shuffle(data_test, labels_test)\n",
    "\n",
    "print( \"Standard test data shape: \" + str( data_test.shape ), flush=True)\n",
    "print( \"Standard test labels shape: \" + str( labels_test.shape ), flush=True)\n",
    "\n",
    "path_to_save_dir = \"/global/home/users/rrmastandrea/training_data/\"\n",
    "\n",
    "save_id = \"STANDARD_TEST_SET_n_sig_10k_n_bkg_10k\"+\"_n_nonzero_\"+str(n_nonzero_conts)+\"_n_pad_\"+str(n_zero_pad)+\"_n_jet_\"+str(j_per_e)+\"/\"\n",
    "\n",
    "path_to_save_dir += save_id\n",
    "print(path_to_save_dir)\n",
    "\n",
    "if os.path.isdir(path_to_save_dir):\n",
    "    print(\"ERROR: experiment already exists, don't want to overwrite it by mistake\")\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(path_to_save_dir)\n",
    "    \n",
    "np.save(path_to_save_dir+\"data.npy\",data_test)\n",
    "np.save(path_to_save_dir+\"labels.npy\",labels_test)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# split into train - val \n",
    "test_size = .3\n",
    "((sig_train, sig_test),\n",
    " (bkg_train, bkg_test),\n",
    " ) = [train_test_split(arr, test_size=test_size) for arr in [\n",
    "    dataset_sample_sig,\n",
    "    dataset_sample_bkg,\n",
    "]]\n",
    "\n",
    "\n",
    "# preparing the training dataset w/ labels\n",
    "data_train = np.concatenate([sig_train,bkg_train])\n",
    "labels_train = np.concatenate([np.ones(sig_train.shape[0]),np.zeros(bkg_train.shape[0])])\n",
    "data_train, labels_train = shuffle(data_train, labels_train)\n",
    "\n",
    "# preparing the test dataset(s)\n",
    "data_test = np.concatenate([sig_test,bkg_test])\n",
    "labels_test = np.concatenate([np.ones(sig_test.shape[0]),np.zeros(bkg_test.shape[0])])\n",
    "data_test, labels_test = shuffle(data_test, labels_test)\n",
    "\n",
    "# Split the test into val + \"testf\"\n",
    "lct_val_size = 1\n",
    "n_val = int(data_test.shape[0]*lct_val_size)\n",
    "data_val = data_test[0:n_val,:,:]\n",
    "labels_val = labels_test[0:n_val]\n",
    "data_test_f = data_test[n_val:,:,:]\n",
    "labels_test_f = labels_test[n_val:]\n",
    "\n",
    "\n",
    "# print data dimensions\n",
    "print( \"CLR training data shape: \" + str( clr_train.shape ), flush=True)\n",
    "print( \"CLR val data shape: \" + str( clr_val.shape ), flush=True)\n",
    "print( \"BC training data shape: \" + str( data_train.shape ), flush=True)\n",
    "print( \"BC training labels shape: \" + str( labels_train.shape ), flush=True)\n",
    "print( \"BC val data shape: \" + str( data_val.shape ), flush=True)\n",
    "print( \"BC val labels shape: \" + str( labels_val.shape ), flush=True)\n",
    "print( \"BC test data shape: \" + str( data_test_f.shape ), flush=True)\n",
    "print( \"BC test labels shape: \" + str( labels_test_f.shape ), flush=True)\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print( \"time taken to load and preprocess data: \"+str( np.round( t1-t0, 2 ) ) + \" seconds\", flush=True  )\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1ad8f-d110-4896-b89e-e69e603adaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_dir = \"/global/home/users/rrmastandrea/training_data/\"\n",
    "\n",
    "\n",
    "#n_sig_total = dataset_clr_sig.shape[0]\n",
    "#n_bkg_total = dataset_clr_bkg.shape[0]\n",
    "\n",
    "n_sig_total = dataset_sample_sig.shape[0]\n",
    "n_bkg_total = dataset_sample_bkg.shape[0]\n",
    "\n",
    "\n",
    "dataset_sample_sig\n",
    "\n",
    "\n",
    "save_id_dir = \"nBC_sig_\"+str(n_sig_total)+\"_nBC_bkg_\"+str(n_bkg_total)+\"_n_nonzero_\"+str(n_nonzero_conts)+\"_n_pad_\"+str(n_zero_pad)+\"_n_jet_\"+str(j_per_e)+\"/\"\n",
    "\n",
    "path_to_save_dir += save_id_dir\n",
    "print(path_to_save_dir)\n",
    "\n",
    "if os.path.isdir(path_to_save_dir):\n",
    "    print(\"ERROR: experiment already exists, don't want to overwrite it by mistake\")\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(path_to_save_dir)\n",
    "    \n",
    "np.save(path_to_save_dir+\"clr_train.npy\",clr_train)\n",
    "np.save(path_to_save_dir+\"clr_val.npy\",clr_val)\n",
    "np.save(path_to_save_dir+\"data_train.npy\",data_train)\n",
    "np.save(path_to_save_dir+\"labels_train.npy\",labels_train)\n",
    "np.save(path_to_save_dir+\"data_val.npy\",data_val)\n",
    "np.save(path_to_save_dir+\"labels_val.npy\",labels_val)\n",
    "#np.save(path_to_save_dir+\"data_test_f.npy\",data_test_f)\n",
    "#np.save(path_to_save_dir+\"labels_test_f.npy\",labels_test_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca05eca-25ef-4c41-94bb-404aaae36882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffee9e5-937c-475d-ab53-ba53f2f6dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b948293-9a47-477f-a50a-9cb1375a1a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d393ec-4032-4883-9c2b-f59549a851d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f88324d-16ea-4802-8f72-acd1637ee254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5b18c-c754-4d6f-96e2-e6ceb3ea72c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0ff95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
